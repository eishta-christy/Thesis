I N T R O D U C T I O N.  In everyday conversations adults perceive speech by ear and eye, yet the development of this critical audiovisual property of speech perception is still not well understood.  In fact, the extant child research reveals that – compared to adults – children exhibit reduced sensitivity to the articulatory gestures of talkers (i. e.  visual speech).  The McGurk task (McGurk & MacDonald, ) well illustrates this maturational diﬀerence in sensitivity to visual speech.  In this task, individuals are presented with audiovisual stimuli with conﬂicting auditory and visual onsets (e. g.  hear /ba/ and see /ga/).  Whereas adults typically perceive a blend of the auditory and visual inputs (e. g.  /da/ or /ða/) and rarely report perceiving the auditory /ba/, children, by contrast, report perceiving the /ba/ (auditory capture) % to % of the time (McGurk & MacDonald, ).  Because visual speech plays a role in learning the phonological structure of spoken language (e. g.  Locke, .  Mills, ), it is critical to understand how children utilize visual speech cues.  The influence of visual speech on children’s audiovisual speech perception clearly increases with age, but the precise timecourse for achieving adultlike benefit from visual speech remains unclear.  Numerous studies report that (i) children from roughly five through eleven years of age benefit less than adults from visual speech whereas (ii) adolescents (preteens–teenagers) show an adultlike visual speech advantage (e. g.  Desjardins, Rogers & Werker, .  Dodd, .  Erdener & Burnham, .  Jerger, Damian, Spence, Tye-Murray & Abdi, .  McGurk & MacDonald, .  Ross, Molholm, Blanco, Gomez-Ramirez, Saint-Amour & Foxe, .  Tremblay, Champoux, Voss, Bacon, Lepore & Theoret, .  Wightman, Kistler & Brungart, ).  Developmental improvements in sensitivity to visual speech have been attributed to changes in (i) the perceptual weights given to visual speech (Green, ), (ii) articulatory proficiency and/or speechreading skills (e. g.  Desjardins et al. , .  Erdener & Burnham, ), and (iii) linguistic skills and language-specific tuning (Erdener & Burnham, .  Sekiyama & Burnham, ).  Notable complications to this story are suggested, however, by several studies reporting significant sensitivity to visual speech in three- to five-year-olds (Holt, Kirk & Hay-McCutcheon, .  Lalonde & Holt, ), six- to seven-year-olds (Fort, Spinelli, Savariaux & Kandel, ), and eight-year-olds (Sekiyama & Burnham, , ).  Some of these studies stressed that performance in young children can be influenced by visual speech when the children are tested with developmentally appropriate measures and task demands.  This viewpoint encourages us to consider the possible bases underlying children’s developmental insensitivity to visual speech.  Toward this end, Jerger et al.  () adopted a dynamic systems theoretical viewpoint (Smith & Thelen, ).  Dynamic systems theory.  Dynamic systems theory proposes two relevant points for understanding the influence of visual speech in children.  (i) multiple interactive factors form the basis of developmental change, and (ii) children’s early skills are ‘softly assembled’ systems that reorganize into more mature, stable forms in response to environmental and internal forces (Smith & Thelen, ).  Evoked potential studies support such a developmental reorganization and restructuring of the phonological system (Bonte & Blomert, ).  During these developmental transitions, processing systems are less robust and children cannot easily use their cognitive resources.  thus performance is less stable and more affected by methodological approaches and task demands (Evans, ).  From this perspective, children’s reduced sensitivity to visual speech may be incidental to developmental transformations, their processing by-products, and experimental contexts.  Clearly, previous research has shown a greater influence of visual speech on children’s performance when task demands were modified to be more child-appropriate (Desjardins et al. , .  Lalonde & Holt, ).  Further, sensitivity to visual speech has been shown to vary in the same children as a function of stimulus/task demands (Jerger, Damian,Tye-Murray & Abdi, ).  We propose that some experimental variables that might have contributed to children’s reduced sensitivity to visual speech are the use of (i) complex tasks/ audiovisual stimuli (e. g.  targets embedded in noise or competing speech.  McGurk stimuli with conflicting auditory and visual onsets) – because they make listening more challenging or less natural and familiar – and (ii) high-fidelity auditory speech – because it makes visual speech less relevant.  The purpose of the present research was to evaluate whether sensitivity to visual speech in children might be increased by the use of stimuli with (i) congruent onsets that invoke more prototypical and representative audiovisual speech processes, and (ii) non-intact auditory onsets that increase the need for visual speech without involving noise.  Below we briefly introduce our new stimuli and discuss the current task and its possible benefits for studying the influence of visual speech on performance by children.  Stimuli for the New Visual Speech Fill-In Effect The new stimuli are words and nonwords with an intact consonant/rhyme in the visual track coupled to a non-intact onset/rhyme in the auditory track (our methodological criterion excised about  ms for words and  ms for nonwords.  see ‘Method’).  Stimuli are presented in audiovisual vs.  auditory modes.  Example stimuli for the word bag are.  (i) audiovisual.  intact visual (b/ag) coupled to non-intact auditory (–b/ag) and (ii) auditory.  static face coupled to the same non-intact auditory (–b/ag).  Our idea was to insert visual speech into the ‘nothingness’ created by the excised auditory onset to study the possibility of a Visual Speech Fill-In Effect (Jerger et al. , ), which occurs when performance for the SAME auditory stimulus DIFFERS depending upon the presence/absence of visual speech.  Responses illustrating a Visual Speech Fill-In Effect for a repetition task (Jerger et al. , ) are perceiving /bag/ in the audiovisual mode but /ag/ in the auditory mode.  Below we overview our new approach – the multimodal picture–word task with low-fidelity speech (non-intact auditory onsets).  Multimodal picture–word task In the widely used picture word interference task (Schriefers, Meyer &Levelt, ), participants name pictures while attempting to ignore nominally irrelevant speech distractors.  Previous research (e. g.  Jerger, Martin & Damian, .  Jerger et al. , ) has established that congruent onsets, such as [picture]–[distractor] pairs of [bug]–[bus], speed up picture naming times relative to neutral (or baseline) vowel onsets, such as [bug]–[onion].  A congruent onset is thought to prime picture naming because it creates crosstalk between the phonological representations that support speech production and perception (Levelt, Schriefers, Vorberg, Meyer, Pechmann & Havinga, ).  Congruent distractors are assumed to spread activation from input to output phonological representations, a process fostering faster selection of speech segments during naming (Roelofs, ).  Our ‘multimodal’ version of this task (Jerger et al. , ) administers audiovisual stimuli (Quicktime movie files).  The to-be-named pictures appear on the T-shirt of a talker whose face moves (audiovisual speech utterance) or stays artificially still (auditory speech utterance coupled with still video).  Hence, the speech distractors are presented audiovisually or auditorily only, a manipulation that enables us to study the influence of visual speech on phonological priming.  In a previous study with the multimodal picture–word task and high fidelity distractors (Jerger et al. , ), we observed a U-shaped developmental function with a significant influence of visual speech on phonological priming in four-year-olds and twelve-year-olds, but not in five- to nine-year-olds.  Consistent with our dynamic systems theoretical viewpoint (Smith & Thelen, ), we proposed that phonological knowledge was reorganizing – particularly from five to nine years – into a more elaborated, systematized, and robust resource for supporting a wider range of activities, such as reading.  The phonological knowledge supporting visual speech processing was not as readily accessed and/or retrieved during this pronounced period of restructuring for the reasons elaborated above (see also Jerger et al. , ).  As noted above, our current research attempts to moderate these possible internal/external influences by using congruent audiovisual stimuli with non-intact auditory onsets.  Our focus on speech onsets may be key because – relative to the other parts of an utterance – onsets are easier to speechread, more reliable with less articulatory variability, and more stressed (Gow, Melvold & Manuel, ).  In two studies, we addressed research questions about the relation between phonological priming in the auditory vs.  audiovisual modes as a function of the characteristics of the stimuli (Analysis ) and the children’s ages and verbal abilities (Analysis ).  ANALYSIS .  STIMULUS CHARACTERISTICS.  The general aim of this analysis was to assess the influence of visual speech on phonological priming by high- vs.  low-fidelity auditory speech in children from four to fourteen years.  Whereas the auditory fidelity was manipulated from high to low (intact vs.  non-intact onsets), the visual fidelity always remained high (intact).  Primary research questions were whether – in all age groups – (i) the presence of visual speech would fill in the non-intact auditory onsets and prime picture naming more effectively than auditory speech alone and (ii) phonological priming would display a greater influence of visual speech for non-intact than intact auditory onsets.  Finally, a secondary research question concerned LEXICAL STATUS, namely whether phonological priming in all age groups would display a greater influence of visual speech for nonwords than words (e. g.  baz vs.  bag).  Some important qualities that may influence the effects of visual speech are.  (i) CONGRUENT DIMENSIONS, (ii) INTEGRAL PROCESSING OF SPEECH CUES, and (iii) LOW-FIDELITY AUDITORY SPEECH.  STIMULUS CHARACTERISTICS AND PREDICTIONS.  Congruent dimensions.  Evidence suggests that audiovisual utterances with congruent rather than conflicting McGurk-like dimensions produce different perceptual experiences.  For example, Vatakis and Spence () manipulated the temporal onsets of congruent vs.  conflicting auditory and visual inputs and found that listeners were significantly less sensitive to temporal differences when onsets were congruent.  Brain activation patterns also differ for congruent vs.  conflicting audiovisual speech, with supra-additivity (greater than the sum of unimodal inputs) for the former but sub-additivity for the latter (Calvert, Campbell & Brammer, ).  Congruent dimensions also possess lawful relatedness that produces strong cues that the auditory and visual inputs originated from the same speaker and should be integrated (Stevenson, Wallace & Altieri, ).  Thus, in terms of a multisensory perceptual experience, congruent onsets offer some advantages compared to conflicting onsets.  The data below also clearly indicate that the speech cues of consonant–vowel stimuli are processed integrally.  Integrality of speech cues.  To study the integrality of speech cues, the Garner task () requires participants to (i) attend selectively to a target cue such as a consonant (e. g.  /b/ vs.  /g/) and (ii) try to ignore a non-target cue such as a vowel that is held constant (/ba/ vs.  /ga/) or varies irrelevantly (/ba/, /bi/ vs.  /ga/, /gi/).  Results have shown that irrelevant variation in the vowels interferes with classifying the consonants and vice versa (e. g.  Tomiak, Mullennix & Sawusch, ).  Green and Kuhl () established that this tight coupling between auditory speech cues extends to audiovisual speech cues.  All these results indicate that listeners cannot ignore one speech cue and selectively attend to another.  Instead, listeners perceive the cues integrally.  Results on the Garner task imply that our auditory and visual speech onsets should be processed integrally.  Low-fidelity (non-Intact) auditory speech.  The literature shows a shift in the relative weights of the auditory and visual modes as the quality of the inputs shifts.  To illustrate.  when listening to McGurk stimuli with degraded auditory speech, children with normal hearing respond more on the basis of the intact visual input (Huyse, Berthommier & Leybaert, ).  When the visual input is also degraded, however, the children respond more on the basis of the degraded auditory input.  Children with normal hearing or mild–moderate hearing loss and good auditory word recognition – when listening to conflicting inputs such as auditory /meat/ coupled with visual /street/ – respond on the basis of the auditory input (Seewald, Ross, Giolas & Yonovitz, ).  In contrast, children with more severe hearing loss – and more degraded perception of auditory input – respond more on the basis of the visual input.  Finally, when Japanese individuals listen to high-fidelity auditory input, they do not show a McGurk effect.  but when they listen to degraded auditory input, they do show the effect (Sekiyama & Burnham, .  Sekiyama & Tohkura, ).  These results indicate that the relative weighting of auditory and visual speech is modulated by the relative quality of each input.  Recent neuroscience studies also support this differential weighting, as they reveal that the functional connectivity between the auditory and visual cortices and the superior temporal sulcus (STS, an area of audiovisual integration) changes with input fidelity, with increased connectivity between the STS and the sensory cortex with the higher-fidelity input (Nath & Beauchamp, ).  In short, our auditory and visual speech cues are congruent and should be processed in an integral manner.  The auditory and visual speech inputs should be weighed differentially depending on the quality of the auditory input.  Thus we predict that (i) visual speech will fill in the non-intact auditory onsets and prime picture naming more effectively than auditory speech alone, and (ii) children will be more sensitive to visual speech for non-intact than intact auditory input.  In addition to our primary research questions, a secondary research question evaluated whether lexical status affects children’s sensitivity to visual speech.  LEXICAL STATUS AND PREDICTIONS.  The literature contrasting the McGurk effect for words vs.  nonwords indicates that the McGurk effect occurs for both types of stimuli.  Within this evidence, some results have revealed that lexical status impacts the McGurk effect.  For example, visual speech influences listeners more often when (i) stimuli are words rather than nonwords (Barutchu, Crewther, Kiely, Murphy & Crewther, ) or (ii) the visual input forms a word and the auditory input forms a nonword (Brancazio, ).  By contrast, however, other results have shown a strong McGurk effect for both nonwords and words, with performance not appearing to be influenced by meaningfulness (Sams, Manninen, Surakka, Helin & Katto, ).  With regard to studies assessing the McGurk effect with only word stimuli in isolation, one study (Dekle, Fowler & Funnell, ) observed a strong McGurk effect whereas the other study (Easton & Basala, ) reported no visual influence on performance.  In short, these studies do not provide consistent results or predictions In contrast to the mixed results summarized above, the hierarchical model of speech segmentation (Mattys, White & Melhorn, ) provides unambiguous predictions for words vs.  nonwords.  The model proposes that listeners assign the greatest weight to lexical–semantic content when listening to words.  If the lexical–semantic content is compromised, however, listeners assign the greatest weight to phonetic–phonological content.  If both the lexical–semantic and phonetic–phonological content are compromised, listeners assign the greatest weight to acoustic–temporal content.  It is also assumed that monosyllabic words such as our stimuli (bag) may activate their lexical representations without requiring phonological decomposition whereas nonwords (baz) require phonological decomposition (Mattys, ).  If these ideas generalize to our task, word stimuli should be heavily weighted in terms of lexical–semantic content but nonword stimuli should be heavily weighted in terms of phonetic–phonological content for both the audiovisual and auditory modes.  We predict that children’s sensitivity to visual speech will vary depending on the relative weighting and decomposition of the phonetic–phonological content.  To the extent that a greater weight on phonetics–phonology increases children’s awareness of the phonetic–phonological content and visual speech phonetic cues, we predict that children will show a significantly greater influence of visual speech relative to auditory speech for nonwords than for words.  In agreement with Campbell (), we view visual speech as an extra phonetic resource that adds another type of phonetic feature.  Although we critically evaluate the influence of child factors in Analysis , we plot results as a function of age in Analysis .  To briefly address age, the literature reviewed above predicts that – although benefit from visual speech improves with age – children relative to adults show significantly reduced benefit up to the adolescent years.  We have argued above, however, that performance for our non-intact stimuli will reveal MORE sensitivity to visual speech.  We thus predict that phonological priming effects will show influences of visual speech from four to fourteen years.  <Middle>Participants.  Participants were  native English-speaking children ranging in age from .  to .  (% boys).  The racial distribution was % White, % Asian, % Black, and % Multiracial, with % reporting Hispanic ethnicity.  Participants had normal (age-based when appropriate) hearing sensitivity, visual acuity (including corrected to normal), auditory word recognition (Ross & Lerman, ), articulatory proficiency (Goldman & Fristoe, ), and visual perception (Beery & Beery, ).  Participants were divided into four age groups ( to  children each) based on chronological age (four- to five-year-olds.  M= . , SD = ·.  six- to seven-year-olds.  M= . , SD = ·.  eight- to ten-year-olds.  M= . , SD = ·.  and eleven- to fourteen-year-olds.  M= . , SD = ·).  These groups will be referred to as five-year-olds, seven-year-olds, nine-year-olds, and twelve-year-olds.  Details for the groups are presented in Analysis .  Participants accurately pronounced the onsets of the pictures’ names.  the offsets were also accurately pronounced except for three five-year-olds (who substituted /θ/ for /s/ in gas and geese or omitted /t/ in ghost).  Two five-year-olds had to be taught the names of some pictures (geese, beads, and/or gun).  To ensure that the experimental results were reflecting performance for words vs.  nonwords, participants’ knowledge of the word distractors was tested by parental report and a picture-pointing task.  Thirty-one children had to be taught the meaning of a distractor.  the mean number of unknown distractors averaged · in the five-year-olds, · in the seven-year-olds, and · in the nine- to twelve-year-olds.  Mean naming times for the taught vs.  previously known words did not differ.  no trials were eliminated.  Materials and instrumentation.  picture–word task.  Pictures and distractors.  The entire set ofmaterials consisted of experimental items ( pictures and  distractors) and filler items ( pictures and  distractors).  The experimental pictures and phonologically related distractors were words/nonwords beginning with the consonants /b/ or /g/ coupled with the vowels /i/, /æ/, /ʌ/, or /o/.  The baseline distractors were words/nonwords beginning with the vowels /i/, /æ/, /ʌ/, or /o/.  Illustrative items for the picture [bug] are [picture]–[word/nonword] pairs of [bug]–[bus/buv] for the phonologically related condition and [bug]–[onion/onyit] for the baseline condition (see ‘Appendix A’ for items, available at http. //dx. doi. org/. / SX).  The word and nonword distractors were constructed to have as comparable phonotactic probabilities as possible.  In brief, the positional segment frequencies for the words vs.  nonwords averaged respectively · vs.  · (adult values) and · vs.  · (child values).  the biphone frequencies averaged · vs.  · (adult values) and · vs.  · (child values) (Storkel & Hoover, .  Vitevitch & Luce, .  see Jerger et al. , , for details).  The filler items were pictures and word/ nonword distractors NOT beginning with /b/ or /g/.  Illustrative filler items are the [picture]–[word/nonword] pairs of [dog]–[cheese/cheeg], [shirt]–[pickle/ pimmel], and [cookies]–[horse/hork].  To emphasize the distinctiveness between the words and nonwords, if a filler item (e. g.  [dog]–[cheese]) was used for the words, its counterpart (e. g.  [dog]–[cheeg]) was not used for the nonwords and vice versa.  This strategy yielded  different picture–distractor filler items each for the words and the nonwords.  Stimulus preparation.  The distractors were recorded at the Audiovisual Recording Lab, Washington University School of Medicine.  The talker was an eleven-year-old boy actor with clearly intelligible speech.  His full facial image and upper chest were recorded.  He started and ended each utterance with a neutral face / closed mouth.  The color video signal was digitized at  frames/s with -bit resolution at a  ×  pixel size.  The auditory signal was digitized at  kHz sampling rate with -bit amplitude resolution.  The utterances were adjusted to equivalent A-weighted root mean square sound levels.  The video track was routed to a high-resolution monitor, and the auditory track was routed through a speech audiometer to a loudspeaker.  The intensity level of the distractors was approximately  dB SPL.  The to-be-named colored pictures were scanned into a computer as -bit PICT files and edited to achieve objects of a similar size on a white background.  Editing the auditory onsets.  We edited the auditory track of the phonologically related distracters by locating the /b/ or /g/ onsets visually and auditorily with Adobe Premiere Pro and Soundbooth (Adobe Systems Inc. , San Jose, CA) and loudspeakers.  We applied a perceptual criterion to operationally define a non-intact onset.  We excised the waveform in  ms steps from the identified auditory onset (first deviations from baseline) to the point in the later waveform for which at least four of five trained listeners heard the vowel as the onset (auditory mode).  This process removed the excised portion of the acoustic signal and left the alignment between the auditory and visual tracks as originally produced by the speaker.  Splice points were always at zero axis crossings.  Using our perceptual criterion, we excised on average  ms (/b/) and  ms (/g/) from the word onsets and  ms (/b/) and  ms (/g/) from the nonword onsets.  Figure  displays the intact vs.  non-intact waveforms for the word bag.  We next formed audiovisual (dynamic face) and auditory (static face) modes of presentation for the stimuli.  In our experimental design, the auditory mode controls for the influence on performance of any remaining coarticulatory cues in the input.  More specifically, we compare results for the non-intact stimuli in the auditory vs.  audiovisual modes.  Any coarticulatory cues in the auditory input are held constant in the two modes.  Thus any influence on picture naming due to articulatory cues should be controlled, and this should allow us to evaluate whether the addition of visual speech influences performance.  Audiovisual and auditory modes.  Stimuli were Quicktime movie files.  For the audiovisual mode, the children saw (i)  ms (experimental trials) or  or , ms (filler-item trials) of the talker’s still face and upper chest, followed by (ii) an audiovisual utterance of one distractor and the presentation of one picture on the talker’s T-shirt five frames before the auditory onset of the utterance (auditory distractor lags picture), followed by (iii)  ms of still face and picture.  For the auditory mode, the child heard the same event but the video track was edited to contain only the talker’s still face.  The onset of the picture occurred in the same frame for the intact and non-intact distracters.  The relationship between the onsets of the picture and the distractor, termed stimulus onset asynchrony (SOA), must also be considered for the picture–word task.  SOA.  Phonologically related distracters typically produce a maximal effect on naming when the onset of the auditory distractor lags the onset of the picture with a SOA of about  ms (Damian & Martin, .  Schriefers et al. , ).  Our SOA was five frames or about  ms (frame size of  ms) as used previously (Jerger et al. , ).  Because the picture remained in the same frame for the intact and non-intact stimuli, however, the auditory non-intact onset altered the target SOA of  ms and the natural temporal synchrony between the visual and auditory speech onsets.  Below we consider these issues.  With regard to altering the SOA, the child literature does not provide evidence about whether the slight temporal shift in the SOA produced by the non-intact onset affects picture naming results.  Our experimental design, however, should provide data that can control for this issue.  To do so, we will compare results for the non-intact stimuli in the auditory vs.  audiovisual modes.  The shift in the auditory onset is held constant in the two modes.  thus any influence on picture naming due to the shift in the auditory onset should be controlled.  This should allow us to evaluate whether the addition of visual speech influences performance.  With regard to altering the temporal synchrony betweenmodes, visual speech normally leads auditory speech (Bell-Berti & Harris, ), but the degree to which visual speech leads varies appreciably (ten Oever, Sack, Wheat, Bien & van Atteveldt, ).  Thus listeners are accustomed to natural variability in this asynchrony.  Adults synthesize visual and auditory speech into a single multisensory event – without any detection of the asynchrony or any effect on intelligibility – when the visual speech leads the auditory speech by as much as  ms (Grant, van Wassenhove & Poeppel, ).  Detecting asynchrony between audiovisual speech inputs (simultaneity judgments) is similar in adults and ten- to eleven-year-oldswhen visual speech leads (Hillock, Powers & Wallace, ). This evidence suggests that the alternation in theSOAproduced by the non-intact onsets will not affect the children’s assimilation of an audiovisual distractor into a single multisensory event.  Below we summarize our final set of materials.  Final set of items.  We administered two presentations of each experimental item (i. e.  baseline, intact, and non-intact distractors) in the audiovisual and auditory modes.  The items were randomly intermixed with the filler items in each mode and formed into four lists (which were presented forward or backward for eight variations).  Each list contained  experimental (%) and  filler-item (%) trials.  The items comprising a list varied randomly under the constraints that (i) no onset could repeat, (ii) the intact and non-intact pairs (e. g.  bag and /–b/ag) could not occur without at least two intervening items, (iii) a non-intact onset must be followed by an intact onset, (iv) the mode must alternate after three repetitions, and (v) all types of onsets (vowel, intact /b/ and /g/, non-intact /b/ and /g/, and not /b/ or /g/) must be dispersed uniformly throughout the lists.  The presentation of items was counterbalanced so that % of items occurred first in the auditory mode and % occurred first in the audiovisual mode.  The number of intervening items between the intact vs.  non-intact pairs (and vice versa) averaged ten items.  Naming responses.  Participants named pictures by speaking into a unidirectional microphone mounted on an adjustable stand.  The utterances were digitally recorded.  To quantify naming times, the computer triggered a counter/timer (resolution less than one ms) at the initiation of a movie file.  The timer was stopped by the onset of the participant’s vocal response into the microphone, which was fed through a stereo mixing console amplifier and  dB step attenuator to a voice-operated relay (VOR).  A pulse from the VOR stopped the timing board via a data module board.  If necessary, the participant’s speaking level, the position of the microphone or child, and/or the setting on the  dB step attenuator were adjusted to ensure that the VOR triggered reliably.  The counter timer values were corrected for the amount of silence in each movie file before the onset of the picture.  Procedure.  The children completed the multimodal picture–word task along with other procedures in three sessions, scheduled approximately ten days apart.  The order of presentation of the word vs.  nonword conditions was counterbalanced across participants in each age group.  Results were collapsed across the counterbalancing conditions.  In the first session, the children completed three of the word (or nonword) lists.  in the second session, the children completed the fourth word (or nonword) list and the first nonword (or word) list.  and in the third session, the children completed the remaining three nonword (or word) lists.  Individual lists were administered in separated listening conditions.  A variable number of practice trials preceded the presentation of each list.  At the start of the first session, a tester showed each picture on a ” x ” card and asked the participant to name the picture.  the tester taught the target names of any pictures named incorrectly.  Next the tester flashed some picture cards quickly and modeled speeded naming.  The child copied the tester.  Speeded naming practice trials went back and forth between tester and child until the child was naming the pictures fluently.  Mini-practice trials started each of the other sessions.  For formal testing, a tester sat at a computer workstation and initiated each trial by pressing a touch pad (out of child’s sight).  The children, with a co-tester alongside, sat at a distance of  cm directly in front of an adjustable height table containing the computer monitor and loudspeaker.  Trials that the co-tester judged flawed (e. g.  child squirmed out of position, child triggered microphone with non-speech) were deleted online and re-administered after intervening items.  The children were told they would see and hear a boy whose mouth would sometimes be moving and sometimes not.  For the words, participants were told that they might hear words or nonwords.  for the nonwords, participants were told that they would always hear nonwords.  We emphasized that the talking was not important.  Participants were told to focus only on (i) watching for a picture that would pop up on the boy’s T-shirt and (ii) naming it as quickly and as accurately as possible.  The participant’s view of the picture subtended a visual angle of ·° vertically and ·° horizontally.  the view of the talker’s face subtended a visual angle of ·° vertically (eyebrow – chin) and ·° horizontally (eye level).  Finally, participants also completed an explicit repetition task (always presented after the completion of the picture– word task) to assess the perception of the distractor onsets.  RESULTS.  Preliminary analyses.  ‘Appendix B’ (available at http. //dx. doi. org/. /SX) details (i) the accuracy of perceiving the onsets and (ii) the quality of the picture–word data (e. g.  number of missing trials).  In addition to these results, we analyzed the picture–word data preliminarily to determine whether results could be collapsed across the different distractor onsets (/b/ vs.  /g/).  Appendix C (available at http. //dx. doi. org/. /SX) details these results.  Briefly, separate factorial mixed-design analyses of variance were performed for the baseline and phonologically related distractors.  Findings indicated that the different onsets influenced results for the phonologically related distractors but not for the baseline distractors.  Specifically, overall picture naming speed was facilitated slightly more for the /b/ than /g/ onset (– vs.  – ms).  The effect of the onsets was also slightly more pronounced for the audiovisual than auditory mode ( vs.   ms).  Despite these statistically significant outcomes, the differences in performance due to onset were small and did not interact with lexical status (words vs.  nonwords) or fidelity (intact vs.  non-intact).  Thus, we developed a dual-pronged approach.  For the primary analyses below, naming times were collapsed across the onsets to make the principal story clearer.  For one key analysis with the collapsed onsets, however (determining whether/how visual speech influenced performance by assessing the difference between each pair of audiovisual–auditory naming times), the analysis was repeated separately for the individual /b/ and /g/ onsets.  This analysis provides strong evidence for readers interested in whether/how the speechreadability of the onsets influenced phonological priming (e. g.  the bilabial /b/ is easier to speechread than the velar /g/.  Tye-Murray, ).  Baseline picture–word naming times.  Figure  shows average picture naming times for the age groups in the presence of the vowel-onset baseline distractors presented in the auditory or audiovisual modes for the words (left) and nonwords (right).  Results were analyzed with a factorial mixed-design analysis of variance with one between-participants factor (four age groups) and two within-participant factors (lexical status [words vs.  nonwords] and mode [auditory vs.  audiovisual]).  Results indicated that picture naming times decreased significantly as age increased (F(,) = ·,MSE = ·, p < ·, partial η = ·).  No other significant effect was observed.  Picture naming times declined from about  ms in the five-year-olds to  ms in the twelve-year-olds for both words and nonwords in both modes.  This finding agrees previous findings (e. g.  Brooks &MacWhinney, .  Jerger et al. , ).  Phonologically related picture–word naming times.  We quantified the priming produced by the phonologically related distractors on picture naming with adjusted naming times, derived by subtracting each participant’s baseline naming times from his or her phonologically related naming times as in previous studies (e. g.  Jerger et al. , ).  Figure  depicts the adjusted naming times in the age groups for words and nonwords (top vs.  bottom panels) in the auditory and audiovisual modes.  Performance is shown for both the intact and non-intact stimuli (left vs.  right panels).  Results were analyzed with a factorial mixed-design analysis of variance with one between-participants factor (four age groups) and three within-participant factors (lexical status [words vs.  nonwords], fidelity [intact vs.  non-intact], and mode [auditory vs.  audiovisual]).  Table A summarizes the results (significant results are bolded).  All four main factors significantly influenced how the phonologically related distractors primed OVERALL picture naming times, with an effect of (i) AGE GROUP, showing greater priming in the younger than the older children [five-year-olds.  – ms.  seven-year-olds.  – ms.  nine-year-olds and twelve-year-olds.  – ms], (ii) LEXICAL STATUS, showing greater priming from the nonword than the word distractors [respectively – ms vs.  – ms], (iii) FIDELITY, showing greater priming from the intact than the non-intact distractors [respectively – ms vs.  – ms], and (iv) MODE, showing greater priming from the audiovisual than the auditory distractors [respectively – ms vs.  – ms].  The significantly greater priming for the audiovisual mode is particularly relevant because this pattern highlights a significant influence of visual speech on performance.  A few interactions were also significant, but only one involved age group, namely an AGE GROUP X FIDELITY interaction (see Table A).  As shown in Figure  and noted above, the intact (high-fidelity) distractors primed OVERALL picture naming more effectively than the non-intact (low-fidelity) distractors (compare right vs.  left panels collapsed across mode and lexical status).  This interaction arose because the relative effectiveness of the intact vs.  non-intact distractors differed more in the five-year-olds (– ms) than in the older groups (seven-year-olds.  – ms.  nine-year-olds.  – ms.  twelve-year-olds.  – ms).  The other significant interactions (two-way and three-way) shown in Table A involved mode.  To clarify these interactions – and determine whether visual speech significantly influenced performance – we quantified the difference between each pair (audiovisual– auditory) of adjusted naming times.  For the sake of simplicity, we labeled all of the difference scores, for both the intact (high-fidelity) and non-intact (low-fidelity) stimuli, a VISUAL SPEECH EFFECT (VSPE) for these analyses.  We should emphasize, however, that this VSPE is reflecting an actual filling in of some missing auditory cues for non-intact speech and, by contrast, an augmenting of auditory cues for intact speech.  The difference scores are plotted in Figure  and represent the difference between the lines in Figure .  The error bars show the % confidence intervals for the difference scores.  Note that the confidence intervals do not provide relevant information about the intact and non-intact conditions because only difference scores are interpretable for factors that are not independent.  The higher order (MODE X FIDELITY X LEXICAL STATUS) interaction occurred because the VSPE for the non-intact onsets (Figure  collapsed across age groups) was greater for the nonwords than the words (i. e.  respectively  ms vs.   ms.  left vs.  right panels) whereas the VSPE for the intact onsets did not differ for the nonwords vs words (i. e.  respectively  ms vs.   ms).  Although this higher-order interaction may limit the interpretation of the lower-order interactions, we should nonetheless acknowledge the interactions between mode vs.  fidelity and vs.  lexical status.  The MODE X FIDELITY interaction occurred because results showed a greater VSPE for the non-intact than intact onsets (respectively – ms vs.  – ms.  Figure  collapsed across age groups and lexical status).  The MODE X LEXICAL STATUS interaction emerged because results showed a larger VSPE for nonwords than words (respectively – ms vs.  – ms.  Figure  collapsed across age groups and fidelity).  With regard to whether visual speech significantly influenced performance, the confidence intervals (Figure ) address whether a given group showed a significant VSPE (i. e.  did each result differ significantly from zero. ).  If the % confidence interval, or the range of plausible difference scores, does not contain zero, then the results are significant.  The confidence intervals revealed a significant VSPE for all the non-intact and intact onsets excepting one, namely intact nonwords in the five-year-olds.  Finally, confidence intervals for the results in Figure  are also of interest in terms of whether the phonologically related distractors significantly primed naming in each group.  Our specific question was whether each adjusted naming time (difference score between phonologically related naming time and baseline naming time) in each group for each mode differed significantly from zero.  Table  shows the % confidence intervals.  Results indicated significant priming – the confidence interval did not contain zero – for all datapoints in Figure  excepting one.  namely non-intact words, auditory mode in the nine-year-olds.  Although values outside of % confidence intervals are relatively implausible, the lower limits neared zero for two significant results – non-intact nonwords, auditory mode in the nine-year-olds and twelve-year-olds – a pattern suggesting that we should have a lesser degree of confidence in the repeatability of these two outcomes.  With regard to the above effects of age, a complication is that the differences in the baseline naming times muddle an unequivocal interpretation of the results.  In other words, the greater priming effects in the five-year-olds (Figure ) could be a result of age or of these children’s slower baseline naming times.  A straightforward approach to controlling the baseline differences (see Damian & Dumay, ) is to develop priming proportions.  Thus we divided each participant’s adjusted naming times by her or his corresponding baseline naming times (i. e.  [mean time in the phonologically related condition minus mean time in the baseline condition] divided by [mean time in the baseline condition]).  A factorial mixed-design analysis of variance on these transformed data, with the same between- and within-participant factors, yielded the same pattern of results as above (see Table B).  We continued to observe the significant effect of (i) AGE GROUP, showing greater priming in the younger than older children [five-year-olds.  –·.  seven-year-olds.  –·.  nine-year-olds and twelve-year-olds.  –·], and the one age group interaction, AGE GROUP X FIDELITY, which was elaborated above.  With regard to the interactions that the VSPE clarified in Figure , the transformed data also continued to reveal the significant higher-order interaction (MODE X FIDELITY X LEXICAL STATUS) and the two lower-order interactions (MODE X FIDELITY and MODE X LEXICAL STATUS).  A third lower-order interaction (LEXICAL STATUS X FIDELITY) also achieved significance (p = ·).  This interaction occurred because the difference between priming for the intact vs.  non-intact stimuli was slightly greater for nonwords than words, with difference scores respectively of · and · for the proportion transformed data (and  vs.   ms for the untransformed data).  Finally, it is of interest to ask whether there was a complete or partial Visual Speech Fill-In Effect.  The previous MODE X FIDELITY interaction indicates that phonological priming by the intact vs.  non-intact distractors differed more for the auditory (– ms vs.  – ms) than audiovisual (– ms vs.  – ms) mode (see Figure ).  Clearly this interaction reflects a robust Visual Speech Fill-In Effect or, as indicated previously, a greater VSPE for the non-intact than intact onsets.  However, the current question is whether the Visual Speech Fill-In Effect was complete or partial (in other words, were the non-intact audiovisual distractors as phonologically effective as their intact counterparts).  To evaluate whether phonological priming differed for the non-intact vs.  intact audiovisual distractors, we carried out orthogonal contrasts (Abdi & Williams, ) on the mean audiovisual adjusted naming times collapsed across the words and nonwords.  We found significantly greater priming from the intact than non-intact audiovisual distractors in all age groups.  (five-year-olds, Fcontrast (,) = ·, MSE = ·, p < ·, partial η = ·.  seven-year-olds, Fcontrast (,) = ·, MSE = ·, p = ·, partial η = ·.  nine-year-olds, Fcontrast (,) = ·, MSE = ·, p = ·, partial η = ·.  twelve-year-olds, Fcontrast (,) = ·, MSE = ·, p = ·, partial η = ·).  Thus even though the Visual Speech Fill-In Effect was robustly effective, the non-intact audiovisual distractors were not as phonologically compelling as their intact counterparts.  VSPE for the individual /b/ and /g/ onsets.  To probe the influence of visual speech as a function of the speechreadability of the onsets, we analyzed the VSPE scores – without collapsing across the onsets – with a factorial mixed-design analysis of variance with one between-participants factor (four age groups) and three within-participant factors (lexical status [words vs.  nonwords], fidelity [intact vs.  non-intact], and onset [b vs.  g]).  There was no significant effect of lexical status nor were there any interactions between lexical status and fidelity or onset.  thus to graph the results, the VSPE for the onsets was collapsed across words and nonwords.  Figure  portrays the collapsed VSPE for the /b/ and /g/ onsets in the high- (intact) and low(non-intact) fidelity conditions in the age groups, along with the % confidence intervals.  The statistical analysis revealed only one significant result involving onset.  a greater VSPE for the /b/ than the /g/ onset (respectively – ms vs.  – ms when collapsed across fidelity) (F (,) = ·, MSE = ·, p < ·, partial η = ·).  The % confidence intervals shown in Figure  indicated a significant VSPE – the confidence interval did not contain zero – for all datapoints excepting one.  namely the intact stimuli with a /g/ onset in the five-year-olds.  In short, Analysis  indicates that phonological priming OVERALL was significantly greater for the audiovisual than auditory mode.  Visual speech produced significantly greater phonological priming in children from four to fourteen years, with all age groups showing a significant effect of visual speech for most conditions.  The influence of visual speech was slightly greater for the /b/ than the /g/ onsets, but phonological priming did not show the pronounced differences that characterize identifying phonemes on direct measures of speechreading (see also Jordan & Bevan, ).  Next, we investigated the effect of child factors on performance as a function of the mode and stimulus fidelity.  ANALYSIS .  To identify the child factors underpinning the VSPE, we analyzed results for the intact vs.  non-intact words and nonwords as a function of the children’s ages and verbal abilities.  Our goal was to determine which of the child factors – among age, vocabulary, phonological awareness, and speechreading (visual only speech recognition) – uniquely contributed to performance.  We defined ‘uniquely’ statistically as the independent contribution of each variable after controlling for the other variables (Abdi, Edelman, Valentin & Dowling, ).  Use of this regression analytic approach, which yields part (aka, semi-partial) correlations, is essential for identifying the critical individual factors underpinning speech perception by children.  We investigated two basic research questions.  Is the VSPE supported by the same unique child factors for (i) intact vs.  non-intact stimuli and (ii) words vs.  nonwords.  There is little to no evidence to assist in predicting these results.  However, we can predict the effects of child factors from models of the picture–word task.  As noted in the ‘Introduction’, the model of Levelt et al.  () based on auditory distractors proposes that the phonologically related distractor (e. g.  [picture]–[distractor] pair of [bug]–[bus]) primes picture naming by creating crosstalk between the input and output phonological representations supporting speech perception and production.  The congruent distractor activates input phonological representations whose activation spreads to activate the corresponding output phonological representations, and this crosstalk speeds selection of the output speech segments for naming (Roelofs, ).  These models – to the extent they generalize – predict that the quality of children’s phonological representations or knowledge will influence performance on our task.  Again, we view visual speech as an extra phonetic resource as proposed by Campbell ().  Finally, based on the hierarchical model of speech segmentation (Mattys et al. , ), we previously proposed that children’s sensitivity to visual speech will vary depending on their weighting of the phonetic–phonological content.  If this is so, the children’s phonological knowledge may be uniquely important to the VSPE, particularly for nonwords.  In short, the findings below should provide fundamental new knowledge about the contribution of age-related improvements vs.  the absolute excellence of selected verbal skills to speech perception by children. Participants.  Participants were the four groups of Analysis .  Materials and procedure.  Receptive vocabulary was estimated with the Peabody Picture Vocabulary Test (Fourth Edition.  Dunn & Dunn, ), measuring children’s ability to identify a picture illustrating a spoken word’s meaning.  Phonological awareness was estimated with three subtests of the Pre-Reading Inventory of Phonological Awareness (Dodd, Crosbie, McIntosh, Teitzel & Ozanne, ), measuring children’s ability to isolate onset phonemes, recognize alliterative onset phonemes, and segment the phonemes within a word.  Speechreading was estimated with the Children’s Audio-Visual Enhancement Test (Tye-Murray & Geers, ), measuring children’s ability to repeat words presented in the visual (and auditory) modes.  Results for the auditory mode were not reported because all age groups performed at ceiling.  Results for the visual mode were scored by words and by word onsets with visemes (visually indistinguishable phonemes) counted as correct.  The latter results were used to quantify speechreading for the regression analyses.  RESULTS.  Descriptive statistics for child factors.  Table  summarizes the average ages along with selected verbal skills in the groups.  Vocabulary knowledge in the groups averaged about  standard score, a result indicating that these children had higher than average verbal skills.  Although high verbal performance is, in general, typical of children in research studies, such performance could potentially affect the generalizability of the results to children with more ‘average’ verbal abilities.  Phonological awareness averaged % correct in the youngest group and about % correct in the other groups.  performance ranged from the ceiling in all groups to a floor of about % in the five-year-olds, % in the seven-year-olds and nine-year-olds, and % in the twelve-year-olds.  Speechreading ranged, on average, from % to % across groups when scored by words and % to % when scored by wordAssociation between VSPE and child factors.  The goal of this project was explanatory – thus we focused on understanding which of the child factors, if any, contributed significantly to the VSPE when the effects of the other factors were controlled.  To assess the relative importance of each factor in determining the VSPE, we conducted four regression analyses ((i) words–intact, (ii) words–non-intact, (iii) nonwords– intact, and (iv) nonwords–non-intact) to obtain the part (aka semi-partial) correlation coefficients and partial F statistics (Abdi et al. , ).  The dependent variable was always the VSPE, and the independent variables were always the standardized scores for age, vocabulary, phonological awareness, and speechreading.  Table  summarizes these regression results, along with the slope coefficients, for the intact vs.  non-intact conditions (left vs.  right panels) of the words vs.  nonwords (top vs.  bottom panels).  Results for the part correlations reflected one overall pattern for the intact stimuli and the non-intact words.  the VSPE was uniquely influenced by the children’s phonological skills.  In contrast to this pattern of results, the VSPE for the low-fidelity (non-intact) nonwords was uniquely influenced only by speechreading skills.  In short, these results indicate that the VSPE is underpinned by phonological skills unless the input is an unfamiliar low-fidelity stimulus without a lexical representation, in which case speechreading skills become uniquely contributory.  <Conclusion> DISCUSSION.  This research assessed the influence of visual speech on phonological priming by high- vs.  low-fidelity auditory speech in children between four and fourteen years.  The low-fidelity stimuli were words and nonwords with a visual consonant + rhyme coupled to an auditory non-intact onset + rhyme.  Our research paradigm presented the stimuli in the auditory and audiovisual modes to determine whether (i) the presence of visual speech would fill in the non-intact auditory onsets and prime picture naming more effectively than auditory speech alone and (ii) phonological priming would display a greater influence of visual speech for non-intact than intact auditory onsets.  The results showed a significant VSPE not only for the non-intact, but also for the intact, onsets – a pattern indicating that visual speech not only filled in the non-intact auditory cues but also supplemented the intact auditory cues.  We observed a consistently significant influence of visual speech on phonological priming for children of all ages between four to fourteen years for most conditions.  The significant boost by visual speech was substantial, particularly for the non-intact stimuli.  about  ms (intact) and  ms (non-intact).  Results assessing lexical status indicated that the nonwords reflected significantly greater priming OVERALL than the words (respectively – ms vs.  – ms).  However, the lexical status of stimuli interacted with the mode and fidelity.  Results showed that the VSPE for non-intact onsets was significantly greater for nonwords than words (respectively  ms vs.   ms), whereas the VSPE for intact onsets did not differ significantly for the nonwords vs.  words (respectively  ms vs.   ms.  Figure  collapsed across age groups).  A greater VSPE for the non-intact nonwords than words is consistent with our predictions.  When auditory speech has low fidelity, visual speech assumes a relatively greater weight and thus affects performance more.  When this relatively greater weighting of visual speech is coupled with the relatively greater weighting of the phonetic– phonological content for nonwords, a significantly greater influence of visual speech is observed for nonwords than words.  With regard to the higher-order interaction – the VSPE differed for non-intact, but not for intact, words vs.  nonwords – we should note that our set of onsets was constrained (word or nonword stimuli consisting of /b/ and /g/ onsets along with filler and baseline items).  Thus, it is possible that all of the INTACT word/nonword onsets in this limited set had sufficient sensory input for correct perception, and this would yield no difference in performance for the intact words vs.  nonwords.  Results for the multiple comparisons – in all age groups – indicated significantly greater priming for the audiovisual than the auditory mode not only for all non-intact but also for all intact conditions excepting intact nonwords in the five-year-olds.  A worthy question is.  Why did these results – in contrast to the literature – show a significant VSPE for intact stimuli in all age groups.  One possibility is that the variability introduced by intermixing the fidelity (intact vs.  non-intact) and mode (audiovisual vs.  auditory) of the stimuli may have increased children’s awareness of the sensory qualities of the input – thus making visual speech more potent.  Results on the Garner task clearly indicate that participants – when they classify consonants – find it harder to ignore irrelevant inputs that vary (/ba/, /bi/ vs.  /ga/, /gi/) vs.  those that are constant (/ba/ vs.  /ga/).  This pattern suggests that the children may have found it harder to ignore speech distractors that varied in both fidelity and mode.  Results on the Garner task would appear to generalize to our task because individuals process speech automatically (even when instructed to attend to picture naming) and implicitly encode and integrally process all speech cues, not just the target cues.  To illustrate, three- to five-year-olds on a talker recognition task identify cartoon characters from their vocal signatures (e. g. 
Together with technological advancements, new forms of linguistic mediation are being developed in the area of media accessibility and audiovisual translation (AVT).  One such form is respeaking, a method of producing real-time subtitles to live television programs using speech recognition (SR) software (Marsh, 2006.  Romero-Fresco, 2011).  Respeaking was ﬁrst used in 2001 by two public service broadcasters.  the BBC in the United Kingdom and VRT in Belgium (Lambourne, 2006).  Since then, this form of AVT has grown tremendously, and it is now a major method used to produce live subtitling on TV.  Respeaking plays an important role in delivering the original spoken text in the form of written subtitles to people who are deaf and hard of hearing as well as language learners and others who use subtitling to support their TV viewing (Eugeni, 2008a.  Romero-Fresco, 2011).  In daily life, many of us probably have encountered subtitles to live programs and wondered how it is possible for the spoken dialogue to be converted into subtitles within a matter of seconds.  The respeaking process is a complex activity (see Figure 1), in both technical and cognitive terms (Boulianne et al. , 2009.  Luyckx, Delbeke, Van Waes, Leijten, & Remael, 2010.  Romero-Fresco, 2011, 2012).  A respeaker needs to listen to what the original speaker is saying in a TV program and to respeak it, that is, repeat or rephrase the text, adding the necessary punctuation marks and important information for viewers who are deaf or hard of hearing, related to speaker identiﬁcation and sounds.  The words uttered by the respeaker are then turned into text using SR software.  This text is later displayed on viewers’ screens as subtitles with a delay of several seconds (Ofcom, 2015).  In some countries, respeakers are also required to correct the output of the SR program if they spot any errors.  In other countries, the error correction is done by another person, known as editor or moderator.  The error correction process improves the quality of respoken subtitles, but it may increase a delay between the time when the original speaker said something in a TV program and when the corresponding subtitles appeared.  Since delay is one of the most frequently voiced complaints by deaf and hard of hearing viewers (Mikul, 2014), every effort has to be taken to ensure that this delay be as short as possible.  Given that respeaking is a relative newcomer on the AVT scene, both as an AVT practice and as an area of academic research, there are still a number of fundamental questions about respeaking that remain unanswered.  They include, but are not limited to, the similarities and differences between respeaking and interpreting, the competences of respeakers, and the methods of respeaker training aimed at achieving good quality of live subtitling, particularly when it comes to reducing the delay (Mikul, 2014).  In this paper, we address the previously unexplored temporal aspects of respeaking that affect the delay in live subtitles.  ear–voice span (EVS) and pauses.  We examine how the characteristics of to-be-respoken materials modulate temporal aspects of respeaking performance.  In addition, by examining the common ground that respeaking shares with interpreting, we hope to start a discussion on respeaker competences and to better understand the respeaking process.  This, we believe, can in turn translate into concrete solutions in respeaking training, mainly aimed at optimzing the delay in live subtitling.  Because intralingual respeaking shares temporal constraints with interpreting and because interlingual respeaking shares the process of transferring messages from one language into another with both interpreting and translation, we wanted to ﬁnd out if interpreters and translators are better predisposed to producing respoken subtitles with the shortest delay possible than average bilinguals without any interpreting or translation experience.  RESPEAKING AS A HYBRID MODALITY.  Despite being a newcomer on the AVT scene, respeaking seems to have some “elder siblings” in the translation/interpreting family.  First of all, respeaking (especially its interlingual variety) can be likened to media interpreting on television, where the interpreter interprets live televised content for the TV audience (Kurz, 2002.  Pignataro, 2011.  Pöchhacker, 2010).  Both respeakers and media interpreters are bound by strict time constraints since they are often instructed to keep as close to the original speaker as possible (Pignataro, 2011) and to translate at a “supersonic pace” (Bros-Brann, 1994, p.  26), which undoubtedly affects their EVS.  Similarly to media interpreting on TV, long EVS is not desirable in respeaking as it increases the delay in displaying subtitles on screen.  Second, the media interpreter addresses two types of audiences at the same time.  on-screen participants (speakers in the TV studio such as a talk-show host and guests) and off-screen participants (TV viewers.  Sergio, 2013).  In the same vein, respeakers deal with the televised dialogue on the one hand, and with their deaf and hard of hearing audience at home, on the other.  Sergio (2013) noted that it is possible for media interpreting and subtitling to coexist in the same TV program.  when the interpreter’s words are subtitled by another person.  We now know that these two roles can be performed by a single person.  an interlingual respeaker.  Sergio (2013) claims that media interpreting “requires additional skills and a new professional proﬁle” (p.  2) and cites BrosBrann (1993, p.  1), stating that it requires “an entirely new mindset compared to everyday practice of conference interpretation and to what all of us have learned and taught in various schools of interpretation. ” We believe the same applies to respeaking as it now calls for new research and training.  Respeaking also shares some ground with simultaneous ﬁlm interpreting (Russo, 2005), found at ﬁlm festivals and other live events.  The common ground lies in a complex audiovisual situation, combining inputs from the visual, oral, and written codes.  Among the similarities shared by simultaneous ﬁlm interpreting, subtitling, and respeaking are also the need to synchronize the translation with the image and to reduce the original text, as well as the fact of being “caught in the relationship between the written and the oral” (Gambier, 2003, p.  178).  Finally, respeaking has often been likened to simultaneous conference interpreting (Eugeni, 2008b.  Marsh, 2004.  Romero-Fresco, 2011).  Both interpreting and respeaking demand excellent multitasking and split attention skills, as they require listening to the original text and rendering it into the same or another language, simultaneously monitoring the output (Jones, 2002.  Pöchhacker, 2004.  Romero-Fresco, 2011).  Unlike interpreters, however, respeakers also need to add the necessary punctuation marks, and condense the original text to ﬁt the limited space of subtitles on the screen.  One of the most important similarities in the process of respeaking and interpreting is what is known as the EVS. Similarly to respeaking, simultaneous interpreting involves concurrent management of two speech channels.  listening to the source language as well as producing and monitoring the target language.  In order to do that, interpreters may to a certain extent produce the target text during pauses in the source text (Barik, 1973.  Goldman-Eisler, 1972.  Goldman-Eisler, Dechert, & Raupach, 1980), but for most of the time (estimated at 70% by Chernov, 1994), they listen and speak at the same time.  Thus, the time lag or delay between the moment the original utterance is spoken out and the moment when the interpreter produces his/her equivalent in the target language, known as the EVS or décalage, is one of the most important features of processing involved in simultaneous interpreting (Lee, 2002.  Pöchhacker, 2004).  According to Timarová, Dragsted, and Hansen (2011), EVS “provides insight into the temporal characteristics of simultaneity in interpreting, speed of translation and also into the cognitive load and cognitive processing” involved in simultaneous interpreting (p.  121), and it is a reliable and quantiﬁable measure of cognitive processing (Lee, 2002).  It is also an important skill to be practiced in simultaneous interpreter training (Bartłomiejczyk, 2015).  A number of previous studies have attempted to establish a minimum or optimum EVS unit.  EVS can be measured in units of time (e. g. , seconds) and/or linguistic units, such as a number of words or the nature of syntactic phrases.  Paneth (1957) measured EVS values and found they were between 2 and 4 s, a result which was later conﬁrmed by Barik (1973) and Oléron and Nanpon (2002/1965).  Lederer (1978) found that EVS fell between 3 and 6 s.  Other scholars reported the average EVS amounting to 2 s (Christoffels & de Groot, 2004), 2. 68 s (Defrancq, 2015), and 4. 7 s (Timarová et al. , 2011).  Schweda-Nicholson (1987) found EVS to range from 5 to 10 words or several seconds, while Gerver (1969) argued that the average delay at average presentation rates is about 4–5 words.  Goldman-Eisler (1972) suggested that EVS units depend more on the syntax than on the lexis, and established the minimum EVS unit to be a predicative expression (noun phrase and a verb phrase).  Adamowicz (1989) also measured EVS in terms of meaningful syntactic units and nominal/verbal phrases as frequent EVS units in English–Polish interpreting.  Donato (2003) applied an extended typology of syntax-based EVS units and found that the most frequently applied EVS units consisted of a noun phrase and a verb phrase.  Christoffels and de Groot (2004) noticed a certain consistency pertaining to the average EVS duration reported in various studies with varied methodologies.  They claimed that the upper boundary of the EVS is related to memory capacity (longer EVS increases memory load), while the lower boundary is related to the minimum meaningful unit (Lederer, 1981.  Setton, 1999) needed for the interpreter to decode meaning and perform interpreting.  EVS has also been linked to other factors, such as source text delivery rate (Lee, 2002), working with or without text (Lamberger-Felber, 2001), and sentence length (Lee, 2002).  Barik (1973) measured temporal characteristics of interpretation of four types of texts. 
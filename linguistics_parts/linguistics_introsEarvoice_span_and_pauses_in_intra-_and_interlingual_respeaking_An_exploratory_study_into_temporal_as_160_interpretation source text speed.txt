Together with technological advancements, new forms of linguistic mediation are being developed in the area of media accessibility and audiovisual translation (AVT).  One such form is respeaking, a method of producing real-time subtitles to live television programs using speech recognition (SR) software (Marsh, 2006.  Romero-Fresco, 2011).  Respeaking was ﬁrst used in 2001 by two public service broadcasters.  the BBC in the United Kingdom and VRT in Belgium (Lambourne, 2006).  Since then, this form of AVT has grown tremendously, and it is now a major method used to produce live subtitling on TV.  Respeaking plays an important role in delivering the original spoken text in the form of written subtitles to people who are deaf and hard of hearing as well as language learners and others who use subtitling to support their TV viewing (Eugeni, 2008a.  Romero-Fresco, 2011).  In daily life, many of us probably have encountered subtitles to live programs and wondered how it is possible for the spoken dialogue to be converted into subtitles within a matter of seconds.  The respeaking process is a complex activity (see Figure 1), in both technical and cognitive terms (Boulianne et al. , 2009.  Luyckx, Delbeke, Van Waes, Leijten, & Remael, 2010.  Romero-Fresco, 2011, 2012).  A respeaker needs to listen to what the original speaker is saying in a TV program and to respeak it, that is, repeat or rephrase the text, adding the necessary punctuation marks and important information for viewers who are deaf or hard of hearing, related to speaker identiﬁcation and sounds.  The words uttered by the respeaker are then turned into text using SR software.  This text is later displayed on viewers’ screens as subtitles with a delay of several seconds (Ofcom, 2015).  In some countries, respeakers are also required to correct the output of the SR program if they spot any errors.  In other countries, the error correction is done by another person, known as editor or moderator.  The error correction process improves the quality of respoken subtitles, but it may increase a delay between the time when the original speaker said something in a TV program and when the corresponding subtitles appeared.  Since delay is one of the most frequently voiced complaints by deaf and hard of hearing viewers (Mikul, 2014), every effort has to be taken to ensure that this delay be as short as possible.  Given that respeaking is a relative newcomer on the AVT scene, both as an AVT practice and as an area of academic research, there are still a number of fundamental questions about respeaking that remain unanswered.  They include, but are not limited to, the similarities and differences between respeaking and interpreting, the competences of respeakers, and the methods of respeaker training aimed at achieving good quality of live subtitling, particularly when it comes to reducing the delay (Mikul, 2014).  In this paper, we address the previously unexplored temporal aspects of respeaking that affect the delay in live subtitles.  ear–voice span (EVS) and pauses.  We examine how the characteristics of to-be-respoken materials modulate temporal aspects of respeaking performance.  In addition, by examining the common ground that respeaking shares with interpreting, we hope to start a discussion on respeaker competences and to better understand the respeaking process.  This, we believe, can in turn translate into concrete solutions in respeaking training, mainly aimed at optimzing the delay in live subtitling.  Because intralingual respeaking shares temporal constraints with interpreting and because interlingual respeaking shares the process of transferring messages from one language into another with both interpreting and translation, we wanted to ﬁnd out if interpreters and translators are better predisposed to producing respoken subtitles with the shortest delay possible than average bilinguals without any interpreting or translation experience.  RESPEAKING AS A HYBRID MODALITY.  Despite being a newcomer on the AVT scene, respeaking seems to have some “elder siblings” in the translation/interpreting family.  First of all, respeaking (especially its interlingual variety) can be likened to media interpreting on television, where the interpreter interprets live televised content for the TV audience (Kurz, 2002.  Pignataro, 2011.  Pöchhacker, 2010).  Both respeakers and media interpreters are bound by strict time constraints since they are often instructed to keep as close to the original speaker as possible (Pignataro, 2011) and to translate at a “supersonic pace” (Bros-Brann, 1994, p.  26), which undoubtedly affects their EVS.  Similarly to media interpreting on TV, long EVS is not desirable in respeaking as it increases the delay in displaying subtitles on screen.  Second, the media interpreter addresses two types of audiences at the same time.  on-screen participants (speakers in the TV studio such as a talk-show host and guests) and off-screen participants (TV viewers.  Sergio, 2013).  In the same vein, respeakers deal with the televised dialogue on the one hand, and with their deaf and hard of hearing audience at home, on the other.  Sergio (2013) noted that it is possible for media interpreting and subtitling to coexist in the same TV program.  when the interpreter’s words are subtitled by another person.  We now know that these two roles can be performed by a single person.  an interlingual respeaker.  Sergio (2013) claims that media interpreting “requires additional skills and a new professional proﬁle” (p.  2) and cites BrosBrann (1993, p.  1), stating that it requires “an entirely new mindset compared to everyday practice of conference interpretation and to what all of us have learned and taught in various schools of interpretation. ” We believe the same applies to respeaking as it now calls for new research and training.  Respeaking also shares some ground with simultaneous ﬁlm interpreting (Russo, 2005), found at ﬁlm festivals and other live events.  The common ground lies in a complex audiovisual situation, combining inputs from the visual, oral, and written codes.  Among the similarities shared by simultaneous ﬁlm interpreting, subtitling, and respeaking are also the need to synchronize the translation with the image and to reduce the original text, as well as the fact of being “caught in the relationship between the written and the oral” (Gambier, 2003, p.  178).  Finally, respeaking has often been likened to simultaneous conference interpreting (Eugeni, 2008b.  Marsh, 2004.  Romero-Fresco, 2011).  Both interpreting and respeaking demand excellent multitasking and split attention skills, as they require listening to the original text and rendering it into the same or another language, simultaneously monitoring the output (Jones, 2002.  Pöchhacker, 2004.  Romero-Fresco, 2011).  Unlike interpreters, however, respeakers also need to add the necessary punctuation marks, and condense the original text to ﬁt the limited space of subtitles on the screen.  One of the most important similarities in the process of respeaking and interpreting is what is known as the EVS. Similarly to respeaking, simultaneous interpreting involves concurrent management of two speech channels.  listening to the source language as well as producing and monitoring the target language.  In order to do that, interpreters may to a certain extent produce the target text during pauses in the source text (Barik, 1973.  Goldman-Eisler, 1972.  Goldman-Eisler, Dechert, & Raupach, 1980), but for most of the time (estimated at 70% by Chernov, 1994), they listen and speak at the same time.  Thus, the time lag or delay between the moment the original utterance is spoken out and the moment when the interpreter produces his/her equivalent in the target language, known as the EVS or décalage, is one of the most important features of processing involved in simultaneous interpreting (Lee, 2002.  Pöchhacker, 2004).  According to Timarová, Dragsted, and Hansen (2011), EVS “provides insight into the temporal characteristics of simultaneity in interpreting, speed of translation and also into the cognitive load and cognitive processing” involved in simultaneous interpreting (p.  121), and it is a reliable and quantiﬁable measure of cognitive processing (Lee, 2002).  It is also an important skill to be practiced in simultaneous interpreter training (Bartłomiejczyk, 2015).  A number of previous studies have attempted to establish a minimum or optimum EVS unit.  EVS can be measured in units of time (e. g. , seconds) and/or linguistic units, such as a number of words or the nature of syntactic phrases.  Paneth (1957) measured EVS values and found they were between 2 and 4 s, a result which was later conﬁrmed by Barik (1973) and Oléron and Nanpon (2002/1965).  Lederer (1978) found that EVS fell between 3 and 6 s.  Other scholars reported the average EVS amounting to 2 s (Christoffels & de Groot, 2004), 2. 68 s (Defrancq, 2015), and 4. 7 s (Timarová et al. , 2011).  Schweda-Nicholson (1987) found EVS to range from 5 to 10 words or several seconds, while Gerver (1969) argued that the average delay at average presentation rates is about 4–5 words.  Goldman-Eisler (1972) suggested that EVS units depend more on the syntax than on the lexis, and established the minimum EVS unit to be a predicative expression (noun phrase and a verb phrase).  Adamowicz (1989) also measured EVS in terms of meaningful syntactic units and nominal/verbal phrases as frequent EVS units in English–Polish interpreting.  Donato (2003) applied an extended typology of syntax-based EVS units and found that the most frequently applied EVS units consisted of a noun phrase and a verb phrase.  Christoffels and de Groot (2004) noticed a certain consistency pertaining to the average EVS duration reported in various studies with varied methodologies.  They claimed that the upper boundary of the EVS is related to memory capacity (longer EVS increases memory load), while the lower boundary is related to the minimum meaningful unit (Lederer, 1981.  Setton, 1999) needed for the interpreter to decode meaning and perform interpreting.  EVS has also been linked to other factors, such as source text delivery rate (Lee, 2002), working with or without text (Lamberger-Felber, 2001), and sentence length (Lee, 2002).  Barik (1973) measured temporal characteristics of interpretation of four types of texts.  spontaneous speech, semiprepared material, prepared “oral” material (a speech written to be read out), and prepared “written” material (an article).  He found that interpreted texts usually included speaking for a greater proportion of the time than the original texts, and this proportionality was greater for scripted than unscripted texts, as the former have higher information density.  However, due to a low number of participants, Barik did not ﬁnd a consistent pattern of results when it comes to text types.  Timarová et al.  (2011) found a signiﬁcantly smaller EVS when interpreting ﬁgures as opposed to verbs or beginnings of sentences, which means that interpreters try to produce ﬁgures as quickly as possible in order not to burden the memory with such difﬁcult noncontextual items with high informative content (Chmiel, 2015.  Mazza, 2001).  Adamowicz (1989) found smaller EVS in interpreting a spontaneous text as compared to prepared texts, while Díaz-Galaz, Padilla, and Bajo (2015) reported smaller EVS following advance preparation for the simultaneous interpreting task.  It also seems that more experienced interpreters work with smaller EVS, as Timarová, ˇCeˇnková, and Meylaerts (2015) found a negative correlation between median EVS and days of interpreting experience.  Taken together, these studies suggest that EVS depends on a combination of global (such as language combination) and local (such as propositions in the text) factors.  A generally held view is that longer EVS is better than keeping very close to the speaker (Kade, 1967).  Interpreting trainees are taught to prolong their EVS in order to avoid word-for-word interpreting and in order to have enough time to restructure and reformulate the message and to express it naturally in the target language (Bartłomiejczyk, 2015.  Chmiel, 2015.  Gorszczy´nska, 2015).  Shorter EVS is advised when information density in the text increases (e. g. , due to enumerations or numerical data) because it lowers the memory load and leads to fewer omissions.  Empirical studies have shown that shorter EVS does not have to lead to poorer quality (Defrancq, 2015) or even may be associated with better quality and higher accuracy (Lee, 2002.  Timarová et al. , 2014).  It seems that the interpreter has to strike a balance between the EVS that is not too short (so that meaning can be constructed and restructuring is possible) and not too long (so that there is no memory overload leading to lower accuracy of interpretation.  Christoffels, de Groot, & Kroll, 2006.  Kade & Cartellieri, 1971).  In a sense, EVS lengthening and shortening can be the interpreter’s strategic choice and is listed as one of the tactics to be used by interpreters to prevent the occurrence of problems (Gile,As opposed to interpreting, where longer EVS is usually not problematic for the target audience, in respeaking, “décalage is less desirable” (Romero-Fresco, 2011, p.  107) because it causes delay in the live subtitling production process and effectively increases the gap between the on-screen images and the subtitles accompanying them.  Such delay, as mentioned above, is a cause of great distraction to deaf and hard of hearing viewers and makes a TV program with live subtitles difﬁcult to follow as the subtitles simply cannot catch up with the images (Mikul,PAUSES.  Important as it is, EVS is not the only time-related factor that is crucial in respeaking.  Another signiﬁcant factor are pauses made by respeakers in their speech.  In its traditional sense, pauses “serve to divide discourse into tone groups and organize it into information units” (Shlesinger, 1994, p.  229).  some pauses, however, are a result of hesitations (Pignataro, 2011, p.  87).  We can therefore see that some pauses are more desirable than others that are unwanted and can be considered “linguistically detectable faults” (Goffman, 1981, p.  172) and “disﬂuencies” (Garnham, 1985, p.  206).  The undesirable pauses may be an indication of increased cognitive effort on the part of the speaker, or, as Mead (2000) put it, “manifestations of the effort of reasoning and formulation which accompany linguistic production” (p.  91).  Skilled professional speakers are able to control their linguistic output, minimizing the unwanted pauses in production that may “betray moments of doubt or distraction” (Mead, 2000, p.  91).  see also Goffman (1981).  In interpreting, pauses are an important element of ﬂuency, which is thought to be a distinguishing feature between a professional and trainee interpreter (Mead, 2000).  In respeaking, pauses are used as an important element of interaction between the respeaker and the SR software.  In order to work properly, SR requires a short pause (less than 1 s) to be made by respeakers before it will transfer speech into text (Jurafsky & Martin, 2008.  Romero-Fresco, 2011).  Dictating one word at a time is not a good option, as it would remove the language context from the SR process, which is responsible for its high word accuracy (Romero-Fresco, 2011).  Therefore, respeakers need to stop dictating after a few words and make a pause in appropriate moments in order for the SR program to turn the spoken words into text that can then be displayed as subtitles.  It needs to be noted here that subtitling is a constrained medium in itself.  there is a limit to the number of lines and to the number of characters per line that can be used so that people can both read the subtitles and follow the on-screen images.  Given all the above, respeakers are advised to divide the text they respeak into short, self-contained meaningful chunks, known as “respeaking units. ” Romero-Fresco (2011, p.  108) deﬁnes respeaking units as “idea units that lend themselves to accurate recognition by the SR software (phrases as opposed to single words) and to comfortable reading for the viewers (around one line in a one-, two- or three-line subtitle). ” An ideal respeaking unit is believed to consist of ﬁve to seven words (Romero-Fresco, 2011).  Longer stretches of text may cause problems with subtitle segmentation, extending over too many lines, and as such be difﬁcult to follow by viewers.  Pauses, both in respeaking and in interpreting, may result from pauses made by the original speaker and/or the speech production process conducted by the interpreter/respeaker.  In regular speech production, pauses serve a number of functions.  they enable breathing, may have a semantic or rhetorical function, and “provide time to cope with difﬁculties which can arise at any point in the speech production cycle” (Tóth, 2013, p.  3).  In simultaneous interpreting, silent pauses can manifest problems with source text comprehension, lexical search for the translation equivalent, and difﬁculties with expressing a given sense in the target (Bartłomiejczyk, 2006.  Piccaluga, Nespoulous, & Harmegnies, 2005.  Tóth, 2011).  As Piccaluga et al.  (2005, p.  151) state.  “it would be reasonable to think that the proliferation and/or lengthening of these pauses is linked to difﬁculties of various kinds in performing the complex task of interpreting. ” Cecot (2001) examined silent pauses produced by professional interpreters and concluded that the most frequent were segmentation pauses (deﬁned as grammatical and serving a communicative function) followed by hesitation pauses (deﬁned as nongrammatical without a communicative function) and rhetorical pauses.  Cecot’s comparison of pauses in the source and target texts led her to conclude that interpreters generally followed the speaker’s pattern of segmentation and rhetorical pauses.  However, these patterns also reﬂect additional linguistic processing and cognitive effort (Goldman-Eisler, 1972).  This is why studies have shown that there are more pauses in the interpreted text as compared to the source text (Tissi, 2000).  Tissi (2000) found that interpretations included fewer but longer silent pauses than source texts, which suggests that pausing in interpreting might to a large extent stem from linguistic processing that differs from language planning in regular speech production.  Pauses might be strategic (to gain time before a correction) or stem from problems in the simultaneous interpreting technique experienced by interpreting trainees (Tissi, 2000).  Trainees have been found to pause more than professional interpreters, suggesting more cognitive effort expended by trainees for restructuring and lexical retrieval (Tóth, 2013).  Pausing patterns are also inﬂuenced by other factors, such as the direction of interpretation, source text speed, and sound quality.  Interpreters were more ﬂuent and paused less when working into their native language in both simultaneous (Piccaluga et al. 
I N T R O D U C T I O N.  In everyday conversations adults perceive speech by ear and eye, yet the development of this critical audiovisual property of speech perception is still not well understood.  In fact, the extant child research reveals that – compared to adults – children exhibit reduced sensitivity to the articulatory gestures of talkers (i. e.  visual speech).  The McGurk task (McGurk & MacDonald, ) well illustrates this maturational diﬀerence in sensitivity to visual speech.  In this task, individuals are presented with audiovisual stimuli with conﬂicting auditory and visual onsets (e. g.  hear /ba/ and see /ga/).  Whereas adults typically perceive a blend of the auditory and visual inputs (e. g.  /da/ or /ða/) and rarely report perceiving the auditory /ba/, children, by contrast, report perceiving the /ba/ (auditory capture) % to % of the time (McGurk & MacDonald, ).  Because visual speech plays a role in learning the phonological structure of spoken language (e. g.  Locke, .  Mills, ), it is critical to understand how children utilize visual speech cues.  The influence of visual speech on children’s audiovisual speech perception clearly increases with age, but the precise timecourse for achieving adultlike benefit from visual speech remains unclear.  Numerous studies report that (i) children from roughly five through eleven years of age benefit less than adults from visual speech whereas (ii) adolescents (preteens–teenagers) show an adultlike visual speech advantage (e. g.  Desjardins, Rogers & Werker, .  Dodd, .  Erdener & Burnham, .  Jerger, Damian, Spence, Tye-Murray & Abdi, .  McGurk & MacDonald, .  Ross, Molholm, Blanco, Gomez-Ramirez, Saint-Amour & Foxe, .  Tremblay, Champoux, Voss, Bacon, Lepore & Theoret, .  Wightman, Kistler & Brungart, ).  Developmental improvements in sensitivity to visual speech have been attributed to changes in (i) the perceptual weights given to visual speech (Green, ), (ii) articulatory proficiency and/or speechreading skills (e. g.  Desjardins et al. , .  Erdener & Burnham, ), and (iii) linguistic skills and language-specific tuning (Erdener & Burnham, .  Sekiyama & Burnham, ).  Notable complications to this story are suggested, however, by several studies reporting significant sensitivity to visual speech in three- to five-year-olds (Holt, Kirk & Hay-McCutcheon, .  Lalonde & Holt, ), six- to seven-year-olds (Fort, Spinelli, Savariaux & Kandel, ), and eight-year-olds (Sekiyama & Burnham, , ).  Some of these studies stressed that performance in young children can be influenced by visual speech when the children are tested with developmentally appropriate measures and task demands.  This viewpoint encourages us to consider the possible bases underlying children’s developmental insensitivity to visual speech.  Toward this end, Jerger et al.  () adopted a dynamic systems theoretical viewpoint (Smith & Thelen, ).  Dynamic systems theory.  Dynamic systems theory proposes two relevant points for understanding the influence of visual speech in children.  (i) multiple interactive factors form the basis of developmental change, and (ii) children’s early skills are ‘softly assembled’ systems that reorganize into more mature, stable forms in response to environmental and internal forces (Smith & Thelen, ).  Evoked potential studies support such a developmental reorganization and restructuring of the phonological system (Bonte & Blomert, ).  During these developmental transitions, processing systems are less robust and children cannot easily use their cognitive resources.  thus performance is less stable and more affected by methodological approaches and task demands (Evans, ).  From this perspective, children’s reduced sensitivity to visual speech may be incidental to developmental transformations, their processing by-products, and experimental contexts.  Clearly, previous research has shown a greater influence of visual speech on children’s performance when task demands were modified to be more child-appropriate (Desjardins et al. , .  Lalonde & Holt, ).  Further, sensitivity to visual speech has been shown to vary in the same children as a function of stimulus/task demands (Jerger, Damian,Tye-Murray & Abdi, ).  We propose that some experimental variables that might have contributed to children’s reduced sensitivity to visual speech are the use of (i) complex tasks/ audiovisual stimuli (e. g.  targets embedded in noise or competing speech.  McGurk stimuli with conflicting auditory and visual onsets) – because they make listening more challenging or less natural and familiar – and (ii) high-fidelity auditory speech – because it makes visual speech less relevant.  The purpose of the present research was to evaluate whether sensitivity to visual speech in children might be increased by the use of stimuli with (i) congruent onsets that invoke more prototypical and representative audiovisual speech processes, and (ii) non-intact auditory onsets that increase the need for visual speech without involving noise.  Below we briefly introduce our new stimuli and discuss the current task and its possible benefits for studying the influence of visual speech on performance by children.  Stimuli for the New Visual Speech Fill-In Effect The new stimuli are words and nonwords with an intact consonant/rhyme in the visual track coupled to a non-intact onset/rhyme in the auditory track (our methodological criterion excised about  ms for words and  ms for nonwords.  see ‘Method’).  Stimuli are presented in audiovisual vs.  auditory modes.  Example stimuli for the word bag are.  (i) audiovisual.  intact visual (b/ag) coupled to non-intact auditory (–b/ag) and (ii) auditory.  static face coupled to the same non-intact auditory (–b/ag).  Our idea was to insert visual speech into the ‘nothingness’ created by the excised auditory onset to study the possibility of a Visual Speech Fill-In Effect (Jerger et al. , ), which occurs when performance for the SAME auditory stimulus DIFFERS depending upon the presence/absence of visual speech.  Responses illustrating a Visual Speech Fill-In Effect for a repetition task (Jerger et al. , ) are perceiving /bag/ in the audiovisual mode but /ag/ in the auditory mode.  Below we overview our new approach – the multimodal picture–word task with low-fidelity speech (non-intact auditory onsets).  Multimodal picture–word task In the widely used picture word interference task (Schriefers, Meyer &Levelt, ), participants name pictures while attempting to ignore nominally irrelevant speech distractors.  Previous research (e. g.  Jerger, Martin & Damian, .  Jerger et al. , ) has established that congruent onsets, such as [picture]–[distractor] pairs of [bug]–[bus], speed up picture naming times relative to neutral (or baseline) vowel onsets, such as [bug]–[onion].  A congruent onset is thought to prime picture naming because it creates crosstalk between the phonological representations that support speech production and perception (Levelt, Schriefers, Vorberg, Meyer, Pechmann & Havinga, ).  Congruent distractors are assumed to spread activation from input to output phonological representations, a process fostering faster selection of speech segments during naming (Roelofs, ).  Our ‘multimodal’ version of this task (Jerger et al. , ) administers audiovisual stimuli (Quicktime movie files).  The to-be-named pictures appear on the T-shirt of a talker whose face moves (audiovisual speech utterance) or stays artificially still (auditory speech utterance coupled with still video).  Hence, the speech distractors are presented audiovisually or auditorily only, a manipulation that enables us to study the influence of visual speech on phonological priming.  In a previous study with the multimodal picture–word task and high fidelity distractors (Jerger et al. , ), we observed a U-shaped developmental function with a significant influence of visual speech on phonological priming in four-year-olds and twelve-year-olds, but not in five- to nine-year-olds.  Consistent with our dynamic systems theoretical viewpoint (Smith & Thelen, ), we proposed that phonological knowledge was reorganizing – particularly from five to nine years – into a more elaborated, systematized, and robust resource for supporting a wider range of activities, such as reading.  The phonological knowledge supporting visual speech processing was not as readily accessed and/or retrieved during this pronounced period of restructuring for the reasons elaborated above (see also Jerger et al. , ).  As noted above, our current research attempts to moderate these possible internal/external influences by using congruent audiovisual stimuli with non-intact auditory onsets.  Our focus on speech onsets may be key because – relative to the other parts of an utterance – onsets are easier to speechread, more reliable with less articulatory variability, and more stressed (Gow, Melvold & Manuel, ).  In two studies, we addressed research questions about the relation between phonological priming in the auditory vs.  audiovisual modes as a function of the characteristics of the stimuli (Analysis ) and the children’s ages and verbal abilities (Analysis ).  ANALYSIS .  STIMULUS CHARACTERISTICS.  The general aim of this analysis was to assess the influence of visual speech on phonological priming by high- vs.  low-fidelity auditory speech in children from four to fourteen years.  Whereas the auditory fidelity was manipulated from high to low (intact vs.  non-intact onsets), the visual fidelity always remained high (intact).  Primary research questions were whether – in all age groups – (i) the presence of visual speech would fill in the non-intact auditory onsets and prime picture naming more effectively than auditory speech alone and (ii) phonological priming would display a greater influence of visual speech for non-intact than intact auditory onsets.  Finally, a secondary research question concerned LEXICAL STATUS, namely whether phonological priming in all age groups would display a greater influence of visual speech for nonwords than words (e. g.  baz vs.  bag).  Some important qualities that may influence the effects of visual speech are.  (i) CONGRUENT DIMENSIONS, (ii) INTEGRAL PROCESSING OF SPEECH CUES, and (iii) LOW-FIDELITY AUDITORY SPEECH.  STIMULUS CHARACTERISTICS AND PREDICTIONS.  Congruent dimensions.  Evidence suggests that audiovisual utterances with congruent rather than conflicting McGurk-like dimensions produce different perceptual experiences.  For example, Vatakis and Spence () manipulated the temporal onsets of congruent vs.  conflicting auditory and visual inputs and found that listeners were significantly less sensitive to temporal differences when onsets were congruent.  Brain activation patterns also differ for congruent vs.  conflicting audiovisual speech, with supra-additivity (greater than the sum of unimodal inputs) for the former but sub-additivity for the latter (Calvert, Campbell & Brammer, ).  Congruent dimensions also possess lawful relatedness that produces strong cues that the auditory and visual inputs originated from the same speaker and should be integrated (Stevenson, Wallace & Altieri, ).  Thus, in terms of a multisensory perceptual experience, congruent onsets offer some advantages compared to conflicting onsets.  The data below also clearly indicate that the speech cues of consonant–vowel stimuli are processed integrally.  Integrality of speech cues.  To study the integrality of speech cues, the Garner task () requires participants to (i) attend selectively to a target cue such as a consonant (e. g.  /b/ vs.  /g/) and (ii) try to ignore a non-target cue such as a vowel that is held constant (/ba/ vs.  /ga/) or varies irrelevantly (/ba/, /bi/ vs.  /ga/, /gi/).  Results have shown that irrelevant variation in the vowels interferes with classifying the consonants and vice versa (e. g.  Tomiak, Mullennix & Sawusch, ).  Green and Kuhl () established that this tight coupling between auditory speech cues extends to audiovisual speech cues.  All these results indicate that listeners cannot ignore one speech cue and selectively attend to another.  Instead, listeners perceive the cues integrally.  Results on the Garner task imply that our auditory and visual speech onsets should be processed integrally.  Low-fidelity (non-Intact) auditory speech.  The literature shows a shift in the relative weights of the auditory and visual modes as the quality of the inputs shifts.  To illustrate.  when listening to McGurk stimuli with degraded auditory speech, children with normal hearing respond more on the basis of the intact visual input (Huyse, Berthommier & Leybaert, ).  When the visual input is also degraded, however, the children respond more on the basis of the degraded auditory input.  Children with normal hearing or mild–moderate hearing loss and good auditory word recognition – when listening to conflicting inputs such as auditory /meat/ coupled with visual /street/ – respond on the basis of the auditory input (Seewald, Ross, Giolas & Yonovitz, ).  In contrast, children with more severe hearing loss – and more degraded perception of auditory input – respond more on the basis of the visual input.  Finally, when Japanese individuals listen to high-fidelity auditory input, they do not show a McGurk effect.  but when they listen to degraded auditory input, they do show the effect (Sekiyama & Burnham, .  Sekiyama & Tohkura, ).  These results indicate that the relative weighting of auditory and visual speech is modulated by the relative quality of each input.  Recent neuroscience studies also support this differential weighting, as they reveal that the functional connectivity between the auditory and visual cortices and the superior temporal sulcus (STS, an area of audiovisual integration) changes with input fidelity, with increased connectivity between the STS and the sensory cortex with the higher-fidelity input (Nath & Beauchamp, ).  In short, our auditory and visual speech cues are congruent and should be processed in an integral manner. 
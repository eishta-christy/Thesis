I N T R O D U C T I O N.  Children with Specific Language Impairment (SLI) fail to develop expressive and/or receptive oral language at the same rate and/or to the same extent as typically developing children, despite adequate intelligence, peripheral sensory functioning, exposure to language, and no neurological, emotional, or social dysfunction (Bishop, .  Leonard, .  Rice, ).  Compared to same-age peers, these children demonstrate an uneven profile of abilities across domains of language, with most experiencing difficulties in grammatical morphology and syntax, but many also demonstrating difficulties in lexical and pragmatic skills (Leonard, ).  Difficulty in lexical processes underlying spoken word recognition is well documented in SLI.  inefficiency or inaccuracy in establishing, accessing, selecting, and retrieving from stored representations of spoken language (Dollaghan, ).  For example, children with SLI tend to have smaller overall expressive/receptive vocabulary (Hick, Botting & Conti-Ramsden, ) and impaired phonological processing (Briscoe, Bishop & Norbury, ), use fewer different words per utterance (Rice, Redmond & Hoffman, .  Wong, Klee, Stokes, Fletcher & Leonard, b), and are less accurate in nonword repetition (Estes, Evans & Else-Quest, ).  Multiple accounts have been proposed to explain the different language deficits observed in children with SLI, and much research has focused on the grammatical deficit account and the processing deficit account (Joanisse & Seidenberg, .  Leonard, ).  Some researchers have also explored the possibility of deficits in specific mechanisms such as lower-level auditory perception (Corriveau, Pasquini & Goswami, .  McArthur & Bishop, a, b.  Tallal & Piercy, ), but little research to date has focused on auditory perception and word recognition deficits within the same children with SLI (but see Montgomery, ).  The present study responded to this research gap by focusing on lexical access for spoken word recognition in SLI, and whether and how it might be related to auditory perception.  Spoken word recognition and the speech-gating task.  Spoken word recognition occurs when auditory perceptual input activates a single lexical phonological representation.  Activation begins upon receipt of auditory input and is updated as the input unfolds.  The activation seems to be graded according to word- and cohort-related factors, and multiple words can be activated in parallel and actively compete until a single match is selected (McMurray, Samelson, Lee & Tomblin, ).  Word recognition can thus be conceptualized as selective elimination from an activated cohort of potential targets until the ‘isolation point’ – the point within the word for which no other word matches are possible (Marslen-Wilson, ).  Spoken word recognition may be facilitated by developmental restructuring of phonological representations.  That is, ongoing reorganization of the lexicon such that representations are stored for accuracy and expediency (e. g.  Metsala & Walley, .  Walley, Metsala & Garlock, ).  with development, primarily between ages one to eight (Fowler, ), words with overlapping phonological properties are stored more systematically for better differentiation.  Word frequency and neighborhood density (i. e.  the number of acoustically similar words) have been posited to drive lexical restructuring.  High- rather than low-frequency words should have better-developed representations because the former are activated more often.  Words from high-density phonological neighborhoods should be under relatively strong pressure to differentiate from other lexical entries to enable recognition (Charles-Luce & Luce, .  Garlock, Walley & Metsala, ).  Lexical restructuring has been observed across alphabetic languages (Ziegler & Goswami, ) – which map morphemes onto phonemes or phoneme strings – as well as a logographic language that maps morphemes consistently onto syllables (i. e.  Chinese.  Kidd, Shum, Ho & Au, ).  Speech-gating, a behavioral task that estimates the point within spoken words beyond which correct recognition is possible (Allopenna, Magnuson & Tanenhaus, .  Grosjean, ), has been used productively and widely to study spoken word recognition and lexical development models.  In a typical non-contextual forward speech-gating task, spoken words are presented from their onset in segments (or ‘gates’) typically  ms in length, with word identification attempted after hearing each gate.  On each trial, an additional gate is added, with trials only ending when each word has been heard in its entirety.  The accumulated time increment at which recognition occurs without subsequent change of mind (the ‘acceptance point’) is thought to reflect the temporal window over which word recognition occurs.  This temporal window has been suggested to directly reflect the underlying organization of the representation.  well-established structure facilitates expedient word recognition because the cohort of incorrect matches can be rapidly discounted, and less-established representations impede word recognition because the cohort remains protractedly active (Metsala, ).  Thus the speech-gating task has been used to quantify the underlying structure of the phonological representations supporting spoken word recognition.  Consistent with this, Metsala observed that time taken to correctly identify gated words decreased with age, and that word frequency and neighborhood density effects were predicted by lexical restructuring.  These findings have been replicated in other gating studies (Griffiths & Snowling, .  Kidd et al. , .  Metsala, Stavrinos & Walley, ).  Speech-gating in SLI.  Speech-gating has been used for assessing spoken word recognition in SLI.  Dollaghan () compared SLI children with typically developing age-matched controls on their recognition of gated familiar and unfamiliar words.  There were no group differences for familiar words, but children with SLI needed to hear more of the unfamiliar words to recognize them than did the controls.  These findings did not support a task-related impairment.  Instead, the deficit seemed specific to the phonological representations of unfamiliar words and the ability to distinguish them from the better-established representations of familiar words.  Accessing less-established representations probably increases processing demands, leaving word recognition vulnerable.  Children with SLI were also less likely to produce the correct initial consonant at the earliest gate for all word types, suggesting lower-level auditory perceptual deficits that impede the initial acoustic-phonetic analysis (Dollaghan, ).  Montgomery () used speech-gating of highly familiar words to examine whether children with SLI were impaired in ‘lexical mapping’ (i. e.  lexical access and lexical activation) – the stage when auditory perceptual input undergoes acoustic-phonetic analysis.  No group differences were found between children with SLI and their typically developing peers matched on either age or vocabulary, arguing against word recognition and low-level auditory perceptual deficits in SLI (Sutherland & Gillon, ).  Mainela-Arnold, Evans, and Coady () likewise observed neither group difference in spoken word recognition isolation points between SLI children and age-matched controls, nor any interaction between SLI status, word frequency, and neighborhood density.  However, children with SLI vacillated more between the target and incorrect cohort items after the isolation point, suggesting that word recognition in SLI is more vulnerable to interference from competing activated representations.  This finding was replicated by Marshall and van der Lely ().  Wong, Kidd, Ho, and Au (a) found that Cantonese-Chinese speaking children with SLI (particularly those with comorbid dyslexia) had significantly later isolation points for high-frequency words, and for high-density words, compared to language-typical peers as well as children who had a history (but no current diagnosis) of SLI.  These findings suggest that the phonological representations of children with SLI have not been restructured to the same extent as those of typically developing children.  Importantly, the subpar speech-gating performance observed in SLI could not be attributed to overall poorer vocabulary because the deficit was limited to high-frequency words and high-neighborhood-density words.  Collectively, the findings on speech-gating and SLI hint at difficulties in some aspects of spoken word recognition in SLI, particularly in Cantonese Chinese.  Wong et al. ’s (a) study stands alone in the literature by reporting frequency and density effects in SLI using speech-gating.  However, given their focus on SLI–dyslexia comorbidity, of the twenty children with a current diagnosis of SLI, only ten had SLI without comorbid dyslexia.  Therefore, the robustness of the findings from speech-gating regarding SLI remains unclear.  The present study therefore set out to take a closer look at these effects, particularly those reported in Wong et al. ’s (a) study, in a much larger sample of children with SLI.  Phonological processing in SLI.  Difficulties in phonological processing skills are well documented in SLI (e. g.  Bishop & Snowling, ).  Snowling () hypothesized that phonological deficits might lead to poorer quality phonological representations, which might in turn impair the development of phonological processing skills.  Poorly established representations would be difficult to access and use, limiting phonological awareness (e. g.  syllable awareness, onset and rime awareness, phoneme deletion, etc. ).  Poorer quality phonological representations might also impair access and retrieval of phonological information, evident via slower and perhaps more errant Rapid Automatized Naming (RAN) performance.  Likewise, the ability to maintain phonological information in short-term memory sufficient to either create or refine phonological representations will likely influence the quality of the representations (Claessen, Leitão, Kane & Williams, ).  There are potentially very informative relations between phonological processing skills and the structure of the underlying representation, particularly in SLI.  However, these relations have not been fully explored using the speech-gating task to quantify the underlying structure of the phonological representation.  To do so was a goal of the present study.  Auditory perception in SLI.  According to McMurray et al.  (), subpar speech-gating suggests more lexical decay – entropy in lexical representations – in SLI that prevents full activation of some, if not all, phonological representations, rendering them vulnerable to active competitors.  This can explain why children with SLI take longer than language-typical controls to reach the acceptance point after the isolation point.  However, it does not explain the difficulties with the initial consonant in SLI reported by Dollaghan ().  an additional account is in order.  One possibility is that children with SLI have low-level auditory perceptual deficits (Dollaghan, .  McArthur & Bishop, .  Rosen, ).  Tallal () reported that ‘language learning impaired’ children were less able to differentiate and/or sequence brief tones (i. e.  different tones at short inter-stimulus intervals, ISI).  Such a deficit in ‘temporal processing’ was hypothesized to inhibit accurate perception of rapid transitions in speech and to impair “the formation of distinct (categorical) phonological representations, leading to delay or disruption in language” (Benasich & Tallal, , p.  ), also implicating phonological processing skills, and potentially reading skills, in this causal chain (e. g.  Tallal, ).  Auditory perception in SLI has been studied using variations of Tallal’s auditory repetition task and other putative ‘temporal’ auditory tasks (e. g.  backward detection/recognition masking, gap detection, tracking of rapid spatial changes.  Rosen, ), with equivocal results.  Importantly, the potential mediating role of phonological representations in the hypothesized relation between auditory perception and phonological processing skills has not been explicitly examined.  No study has yet examined whether both the quality of phonological representations pertinent to spoken word recognition – assessed with speech-gating – and low-level auditory perception are impaired in the same individuals with SLI.  Without such empirical information, these causal relations must remain speculative.  Tallal’s () auditory repetition task inherently conflates frequency discrimination (FD) with general stimulus-based judgments.  Note that people with SLI often struggle with practice trials for the auditory repetition task where tone discrimination, not temporal processing, was stressed (Heath, Hogben & Clark, ), so poorer performance was not limited to short ISIs during the task proper (e. g.  Share, Jorm, McClean & Matthews, ).  Neither is poor performance necessarily limited to ‘temporal’ tasks – indeed Nickisch and Massinger () found reduced performance in SLI only on tasks with an FD component, not those assessing temporal processing.  These results collectively suggest a difficulty with FD in SLI, at times (though not always) exacerbated by task-related pressures.  Poor FD may well turn out to undermine language learning, particularly in tonal languages such as Cantonese Chinese (Wong, Ciocca & Yung, ).  Speech comprises rapid changes in spectral information.  Deficits in fine-grained FD can mean difficulties discriminating spectral patterns in speech signals, which can in turn degrade phonological representations (McArthur & Bishop, a).  There is behavioral, as well as electrophysiological evidence, that at least some children with SLI perform poorly on FD tasks (Hill, Hogben & Bishop, .  McArthur & Bishop, a, b.  .  McArthur, Ellis, Atkinson & Coltheart, .  Mengler, Hogben, Mitchie & Bishop, .  Rinker, Kohls, Richter, Maas, Schultz & Schecker, ), and continue to do so over time (Hill et al. , ).  Nonetheless, because poor FD does not consistently predict SLI, more research is needed to clarify this (Halliday & Bishop, .  Rosen, ).  Accurate speech processing also seems to require sensitivity to modulations in frequency (Frisina, ).  According to Bailey and Snowling (), reduced sensitivity to frequency modulation (FM) when young children are establishing and refining phonological representations may well affect subsequent language ability.  In a typical FM detection task, the modulation rate is fixed and the depth is varied across trials, and the task is to differentiate modulated from un-modulated tones.  Stefanatos, Green, and Ratcliff () found markedly reduced auditory-evokedresponses to FM tones among children with SLI, but Tomblin, Abbas, Records, and Brenneman () could not replicate this.  Bishop, Carlyon, Deeks, and Bishop () speculated that the severity of receptive impairments might be at issue, and their own research revealed a non-significant trend for elevated FM thresholds in SLI, but the thresholds did not predict phonemic awareness.  The exact contribution of FM to speech recognition, then, remains unclear.  <Middle> The present study.  The mixed findings on FD and FM in SLI call for empirical clarification.  Crucially, little research has focused on how spoken word recognition deficits fit with potential auditory perception deficits in SLI.  The present multilevel investigation therefore examined FD, FM, and speech-gating within the same children – children with a diagnosis of SLI, and age-matched controls.  With a relatively large sample, we focused on two main questions.  .  Do children with SLI perform worse than age-matched language-typical controls on spoken word recognition (assessed via speech-gating), frequency discrimination (FD), and frequency modulation (FM).  .  Does performance on the speech-gating task mediate any relation between auditory thresholds, phonological processing, and oral language. Participants.  The sample comprised  native speakers of Cantonese Chinese aged between .  and . , mean .  (years. months), attending kindergarten in Hong Kong.  Fifty-seven ( male,  female) met criteria for SLI (Bishop, .  Leonard, ).  they performed below age-expected levels in receptive and/or expressive language, passed cognitive and hearing screening, and demonstrated no history or signs of neurological or psychosocial problems.  Fifty of them had already been diagnosed with SLI when recruited from government-run Child Assessment Centers via voluntary referral from speech pathologists.  The remaining seven, recruited from local kindergartens, were classified as SLI at the time of testing.  Five senior students in speech–language pathology, under the supervision of an experienced speech–language pathologist, confirmed the SLI status of these fifty-seven children using the Hong Kong Cantonese Oral Language Assessment Scale (HKCOLAS.  T’sou et al. , ) – a standardized test with local norms for diagnosing language impairments in children aged five to twelve.  These children all failed (i. e.  scoring · SD below the mean) two or more of the six subtests in the HKCOLAS, and met the other conventional diagnostic criteria (Leonard, ).  Their nonverbal IQ scores all fell within the normal range on the Raven’s Standard Progressive Matrix according to local age norms (Raven, , Table ).  The fifty-three children ( male,  female) in the language-typical control group were recruited from kindergartens in Hong Kong.  They passed a language screening, scoring no worse than · SD below the age-normed mean on the Cantonese Grammar and the Nominal Expressive Vocabulary subtests of the HKCOLAS – the two subtests recommended for assessing preschool and first-year primary school children when under administration time constraints (To, Cheung & T’sou, ).  In the Cantonese Grammar test, children were asked to match pictures with the sentences heard, answer questions targeting specific grammatical morphemes, and make grammaticality judgments about complex sentences.  For the Expressive Vocabulary subtest, children were shown pictures of objects and asked to name them.  Children in the control group were also given the Narrative Retell subtest, whereby they listened to a story and retold it to a naive listener.  Scoring was based on story content and the use of referring expression, connectives, and complex sentences in the retell.  Non-verbal IQ scores in the control group all fell within the normal range of locally normed Raven’s Standard Progressive Matrix scores (Raven, ).  All children passed a hearing screening of pure tones of ·, , , and  KHz presented at  dB in both ears, measured using a Grason-Stadler GSI model  calibrated audiometer.  No children were reported to have otitis media around the time of testing, or to have a diagnosis of attention deficit and/or hyperactivity disorder, or other psychosocial problems.  All children spoke Cantonese Chinese as their first and dominant language.  Descriptive and comparative statistics for both groups are presented in Table .  Materials.  Psychometric tasks.  .  Phonological processing.  . .  Phonological awareness.  In a syllable-deletion task adopted from Ho, Leung, and Cheung (), children were asked to repeat real Chinese words composed of three syllables each, while deleting one of its syllables from the beginning, middle, or end of the word.  An example, translated to English, would be.  ‘Please say little-fat-pig.  Now say little-fat-pig again, this time without saying fat. ’ Deletion of a syllable results in either a real word or a nonword.  The maximum score for this task was .  . .  Phonological short-term memory.  In a non-word repetition task (Ho et al. , ), children were asked to repeat nonwords with two to six syllables.  These non-words were meaningless combinations of actual Cantonese morphemes, with one syllable per morpheme.  A child received  point for each of the syllables correctly repeated and  point for correct ordering of any two consecutive syllables.  Points were deducted for adding syllables.  The maximum score for this task is .  . .  Rapid Automatized Naming (pictures.  RAN-p).  Children were asked to name five common objects (sun, apple, butterfly, airplane, fan) depicted by line drawings, presented randomly in a  ×  matrix, as quickly and as accurately as possible (Ho et al. , ).  Each child named them from left to right, one row after the other.  Children were asked to complete the task twice, the score being the time taken to name the twenty drawings, averaged across two trials.  Experimental tasks.  .  Frequency Discrimination (FD).  This task was modeled on that used by Hogben and colleagues (Heath, Bishop, Hogben & Roach, .  Hill et al. , .  Mengler et al. , ), and was administered with a Matlab program via headphones on a laptop computer.  In each trial, stimuli were three  ms binaural pure tones with  ms rise/fall and  ms ISI, presented in the AXB format.  The first tone (A) was a  Hz standard, and the third comparison tone (B) had a frequency of  Hz plus the frequency difference for that trial.  The middle tone (X) was the same frequency as either (A) or (B), which varied randomly across trials.  Each child was to indicate, by pressing either ‘’ or ‘’ on the keypad, which of tones A or B matched tone X.  Feedback was given via the presentation of a cartoon ‘sad’ or ‘smiley’ face on the screen.  The frequency difference between the standard and comparison tones was varied between trials via an adaptive -up, -down staircase reversal procedure (Wetherill & Levitt, ), which estimated the % correct performance level.  The initial frequency difference was  Hz.  The initial step-size was  Hz, which halved after each reversal until the minimum of  Hz.  The staircase lasted for eight reversals, and the Just Noticeable Difference (JND) was defined as the average frequency difference of the final four reversals.  Blocks of ten practice trials were first administered (all with a frequency difference of  Hz) until the child achieved a criterion of at least seven out of ten correct.  .  Frequency Modulation detection (FM) This task was modeled on that used by Heath et al.  ().  Stimuli were two  Hz,  ms tones with  ms rise/fall and  ms ISI.  To measure frequency modulation detection, one tone (the standard) was unmodulated, and the other tone (the comparison) was sinusoidally modulated at  Hz.  The presentation order of the standard and comparison was varied randomly across trials.  The task was to indicate, by pressing ‘’ or ‘’ on the keypad, which was the modulated tone (i. e.  the tone that ‘wobbled’).  Modulation depth was varied adaptively across trials using a standard -up, -down adaptive staircase procedure (Wetherill & Levitt, ).  The initial depth was  Hz (i. e.  the frequency ranged from  Hz to  Hz).  The initial step-size was  Hz, which halved after each reversal until a minimum of  Hz.  The staircase lasted for eight reversals, with the threshold defined as the average depth of the final four reversals.  The procedure was otherwise identical to the FD task.  .  Speech-gating.  This task was identical to that described by Wong et al.  (a) and Kidd et al.  ().  The stimuli were sixteen frequently used Cantonese monosyllabic nouns, ranged in length from approximately  ms to  ms (see ‘Appendix’).  They were digitally recorded at a sampling rate of · kHz in a sound-attenuated booth by a native Cantonese-speaking male and stored as . wav files, which were then called into the custom-written Matlab program for the task.  Eight words were high frequency and eight were low frequency, according to word rankings obtained from an on-line database (an approximately , spoken word corpus derived from spontaneous adult conversations and Hong Kong radio programs.  Information on this corpus is available from.  kkluke@ntu.  edu. sg).  Words were ‘high-frequency’ if they ranked above  (i. e.  likely to occur more than  times in every , spoken words), and ‘low-frequency’ if they ranked below .  Cantonese syllables, each represented orthographically by a Chinese logograph (character), display a (Ci)Vt(Ce/G) structure, which generally includes the lexical tone (t) and the nuclear vowel (V) as two obligatory components, and the initial consonant (Ci), the ending consonant (Ce), and the ending glide (G) as optional components (Bauer & Benedict, ).  A few words in Cantonese, however, contain only the nasal consonant ng or m and the tone (e. g.  ng (five)).  In this paper, Cantonese syllables are presented in romanized forms following the scheme adopted by the Linguistic Society of Hong Kong ().  Numerals following the syllables mark one of the nine lexical tones in the language, with each representing a different fundamental frequency pattern.  Syllables with tone  to  end with a vowel, a diphthong, or a nasal consonant, whereas syllables with tone  to  end with a stop consonant /p, t/ or /k/.  Tone  is called the high-level, tone  the mid-level, and tone  the low-level tone.  Tone , , and  are the shortened version of the level tones , , and  respectively.  Level tones have a relatively flat contour where the beginning and the ending fundamental frequency do not change substantially.  Tone , , and  are the contour tones, meaning that these tones have different starting and ending fundamental frequency values.  Tone  is the high-rise, tone  the low-fall, and tone  the low-rise tone.  The same syllable carrying different tones will have different lexical meanings.  For example, ma refers to mother, and ma refers to horse.  The sixteen words used in this task included ten with a CVtC structure (e. g.  jan, sam, beng),  CVtG (e. g.  zai, leoi), and  CV (ce and cyu).  While homophones exist for some of these target words (e. g.  the very low frequency 仁‘kindness’ jan is a homophone variant of the target word 人‘man’ jan), effort was made to ensure that all homophones of the target words were extremely low in frequency (i. e.  below  in ,).  Neighborhood density was estimated for each word by hand because no published database detailing such information was available in Cantonese.  We used traditional definitions of neighborhood density (i. e.  the number of words that overlap with the target word by substituting the onset, nucleus, or final phonemes (Charles-Luce & Luce, ), and we added lexical tone to this list because Cantonese-Chinese is a tonal language.  Estimates ranged from  (low-density) to  (high-density).  delineation of ‘high’ and ‘low’ was based on a median split across the sixteen words.  The task was administered with custom-written Matlab (Version . ) software on an IBM Thinkpad laptop via headphones with a microphone attachment.  A forward-gating, duration-blocked procedure was adopted (Griffiths & Snowling, .  Grosjean, ).  The first block presented the initial gate for each of the sixteen words in random order.  The second block presented the previous gate PLUS ONE GATE for all sixteen words, again in random order.  Successive blocks added another gate such that words were presented in accumulating increments until all sixteen words had been presented in their entirety.  Because words differed from one another in length, they each required a different number of gates.  Thus, blocks had a reducing set of target stimuli.  As in previous studies (Kidd et al. , .  Wong et al. , a), we used an initial gate length of  ms and a subsequent gate length of  ms.  The proportion of the target word presented over the initial gate varied from · for the longest stimulus to · for the shortest (see ‘Appendix’).  Designed to be a child-friendly computer game, each trial showed a picture of aliens on the screen.  A child would first be told that aliens were attempting to learn our language.  the child was to listen to each word segment and repeat the word the alien was trying to say.  The experimenter initiated each trial via a button press.  the speech segment was presented  ms thereafter.  The picture remained on the screen for the entire trial.  The child was given a maximum of  s after the speech segment had ended to verbally identify the target word.  The next trial was initiated once the child had finished speaking, which rarely required the full  s.  Children’s responses were recorded directly on the computer’s hard disk for later coding.  After two practice trials using high-frequency monosyllabic concrete nouns, no rest interval between blocks and no feedback was given.  Administration time was about  minutes.  Upon completion, the experimenter showed images of each noun and asked the child to name it.  This confirmed children’s familiarity with the target words.  General procedure.  This research was approved by The Human Research Ethics Committee at the University of Hong Kong.  Written informed consent was obtained from all parents/guardians prior to participation.  Children also gave verbal assent at the time of testing.  Children were tested in a university laboratory, their kindergarten, or the Child Assessment Centre they attended.  As noted earlier, we used the HKCOLAS to confirm the fifty-seven children’s SLI status.  Two research assistants administered three HKCOLAS subtests (Cantonese Grammar, Nominal Expressive Vocabulary, Narrative Retell) to the control group, and all other tasks (i. e.  the psychometric tasks, the FD, FM, and speech-gating tasks) to the full sample.  Testing took about  hours per child, spread over two or three days.  hearing screening, Raven’s, and HKCOLAS, then the remaining cognitive and experimental tasks.  Parents/guardians received modest cash compensation for participation time and transportation costs to the university.  RESULTS.  Raw scores from the Raven’s test were converted to age-normed standard scores with a mean of  and a standard deviation of , and scores on the HKCOLAS were converted to scaled scores (Table ).  The three oral language subtests of the HKCOLAS correlated highly (Table ) and were therefore combined into one composite score (mean z-score).  Descriptive and comparative statistics for the SLI group and the control group are presented in Table .  Significant differences on the oral language subtests reflected the diagnostic criteria and confirmed the group assignment (with versus without SLI).  There was no significant difference in age between groups.  As expected, the SLI group performed significantly worse than the control group in phonological awareness and non-word repetition.  For non-verbal IQ, one child in each group scored below .  however, their scores (both ) were still within the average range under the classification of the test (Raven, ), and thus were not excluded from the study.  Group means were both over , but the mean non-verbal IQ was significantly lower in the SLI group.  We therefore statistically controlled for non-verbal IQ in data analyses wherever feasible.  Poorer auditory thresholds in SLI.  Thresholds for both auditory tasks were checked for outliers.  none converted to z > · (corresponding to p < ·, two-tailed), therefore none were removed.  For the staircase tracks in both tasks, all but one child (from the SLI group on the FM task) showed an acceptable ‘flattening’ around the threshold following adaptation.  that child’s threshold and associated auditory data were excluded from further analysis.  Table  and Figure  present the descriptive statistics and threshold distributions, respectively.  Thresholds from the FM and FD tasks were significantly related (r = ·, p < ·), suggesting some degree of concurrent validity (Heath et al. , ).  Significant skewness for the FM thresholds was observed in the SLI group (skewness = ·, SE=·), although not the control group (skewness = ·, SE = ·), therefore requiring log transformation.  Skewness in the FD data was not significant for either group.  ANCOVA revealed that children with SLI performed less well (i. e.  had higher thresholds) on both tasks, significantly so for logFM even after controlling for age and non-verbal IQ (F[,] = ·, p = ·, ηp  = ·), but not for FD (F[,] = ·, p > ·, η = ·).  The standard deviation was greatest in the SLI group (Figure ).   children with SLI had a logFM threshold greater than the maximum of the language-typical control group.  This is a classic finding in psychophysical experimentation in clinical groups (Roach, Edwards & Hogben, ).  Do these  children represent a discrete subgroup.  The data given in Table  suggest not.  This subgroup differed significantly from the remaining  children with SLI only in FD thresholds and not on other key variables examined.  Poorer spoken word recognition in SLI.  Verbal responses to the target words of the speech-gating task were coded by two trained coders.  Inter-rater reliability was good, indicated by high intra-class correlations (R = · and · for IP and AP, respectively).  Children’s audio-recorded responses were coded as correct or incorrect at each gate for each target word, thus we identified the gate at which the child first correctly identified the target word (the ‘Isolation Point’.  IP), and correctly identified the word without subsequently changing that response (the ‘Acceptance Point’.  AP).  Both IP and AP were expressed as the mean number of gates required for correct word identification.  The results are presented in Table  and Figure .  Ceiling effects were not observed for any word type (Figure ).  Consistent with Metsala (), children needed to hear more gates for low-frequency (LF) words than high-frequency (HF) words to both recognize and accept them, likewise for low-neighborhood-density (LD) than high-neighborhood-density (HD) words.  To examine the effects of word frequency and neighborhood density on spoken word recognition, IP and AP for HF, LF, HD, and LD words were compared using repeated measures ANOVA, with word type as the within-subject factor, and group (SLI versus control) as the between-subject factor, controlling for age and non-verbal IQ.  Main effects were significant for word type (F[,] = ·, p< ·, η = ·), and marginal for group (F[,] = ·, p = ·, η = ·), and no significant interaction was found between word type and group (F[,] = ·, p = ·, η = ·).  Post-hoc analyses (Tukey’s HSD ps < ·) revealed, as predicted.  (i) the IP occurred at a significantly earlier point in the word than the AP for all word types.  (ii) significantly earlier IP and AP (i. e.  fewer gates) for high-frequency words than low-frequency words.  and (iii) significantly earlier IP and AP for high-density words than low-density words.  To examine whether some word types took less time to accept than others following their initial recognition, the difference between the IP and AP was calculated by word type for all participants.  Absolute values of skewness and kurtosis were all below  for these differences.  Repeated-measures ANOVA, with age and IQ as covariates, indicated a marginally significant effect of word type in these difference scores (F[,] = ·, p = ·, η = ·).  Tukey post-hoc comparisons revealed no significant difference for high- versus low-frequency words.  But there was a significant neighborhood density effect.  children required fewer gates to commit to a word after first identifying the word if it came from a highrather than low-density neighborhood (p < ·).  Considering the pattern of results separately for each group, the main effect of word type was significant for the SLI (F[,] = ·, p < ·, η = ·) but not for the control group (F[,] = ·, p = ·, η = ·).  Among children with SLI, word recognition times indicated by IP and AP were both shorter for high- than low-frequency words, and for highthan low-density words (ps < ·).  For all word types, children with SLI on average needed to hear more of the target words than controls (Table ).  This difference was significant for high-density words after controlling for age and non-verbal IQ (IP.  F[,] = ·, p = ·, ηp  = ·.  AP.  F[,] = ·, p = ·, ηp  = ·), but not for low-density words (IP.  F[,] = ·, p = ·.  AP.  F[,] = ·, p = ·).  Group differences were not found for either high-frequency (IP.  F[,] = ·, p = ·.  AP.  F[,] = ·, p = ·) or low-frequency words (IP.  F[,] = ·, p = ·.  AP.  F[,] = ·, p = ·).  These results suggested that children with SLI needed to hear a greater proportion of high-density words than language-typical controls in order to identify and accept a target word, perhaps reflecting less restructuring for the phonological representations despite the considerable pressure to differentiate.  There was, however, no significant difference between groups in the time between first identifying the word and then committing to it (i. e.  the difference between IP and AP) for any word type (Fs[,] < ·, ps > ·).  We further categorized the word stimuli into four mutually exclusive groups (high-frequency high-density (HF-HD), high-frequency low-density (HF-LD), low-frequency high-density (LF-HD), low-frequency low-density (LF-LD).  Table ) to take a tentative look at how word frequency and neighborhood density might interact to affect word recognition.  No significant differences were observed among the four word categories in either word length (F[,] = ·, p > ·) or the proportion of word presented in the initial gate (F[,] = ·, p > ·).  We performed two separate ANOVAs (one for IP, and one for AP), with group (SLI versus control) as the between-subject factor, and word category (HF-HD, HF-LD, LF-HD, LF-LD) as the within-subject factor, while controlling for age and IQ.  There was a significant main effect of word category for AP (F[,] = ·, p = ·, η = ·) but not IP (F[,] = ·, p = ·, η = ·), and post-hoc comparisons (Tukey’s HSD ps <·) revealed that the earliest AP occurred for the HF-HD words, followed by the LF-HD words, for which word recognition occurred slightly faster than the HF-LD stimuli.  The poorest recognition was on the LF-LD words, which the lexical restructuring hypothesis would predict as under the least developmental pressure to differentiate (Fowler, .  Metsala & Walley, ).  By contrast, the main effects of group (SLI versus control) were not found to be significant for IP and AP (Fs[,] < ·, ps >·), nor were the interaction effects between word category and group (Fs[,] < ·, ps >·).  Does spoken word recognition mediate between auditory perception, phonological processing, and oral language.  Two-tailed Pearson correlations were calculated among all psychometric, auditory, and speech-gating measures (Table ).  LogFM correlated significantly with all oral language and phonological processing measures, but did not correlate with any of the speech-gating variables.  To test for the potential mediation relations among spoken word recognition, auditory perception, and phonological processing, path analyses were conducted using structural equation modeling.  Three competing models were tested, and each model showed one of the three latent variables (word recognition, phonological processing, auditory perception) as a mediator between the other two (Figure , Models –).  In all the models, the auditory perception latent variable was indicated by FD thresholds and logFM, while the phonological processing latent variable was indicated by syllable deletion, non-word repetition, and RAN.  IPs of the four word categories (HF-HD, HF-LD, LF-HD, LF-LD) from the speech-gating task were used to tap the latent variable of word recognition.  Age and IQ were entered as covariates in all the analyses to partial out their effects.  Standardized path coefficients are given in Figure .  Goodness of fit of the models to the data was evaluated by comparing four fit indices, including chi-square statistic, Bentler’s () comparative fit index (CFI), Bollen’s () incremental fit index (IFI), and the root mean square error of approximation (RMSEA).  A significant chi-square (p < ·) indicates that the observed covariance matrix significantly deviates from the covariance matrix implied by the model, and hence suggests poor model fit.  Unlike chi-square, which compares a model with the observed data, both CFI and IFI reflect the relative goodness-of-fit by comparing the fit of a hypothesized model to the fit of a null model.  These indices range from · to ·, with larger values denoting better model fit.  According to the cut-off criteria proposed by Hu and Bentler (), values close to · and above indicate relatively good fit.  RMSEA provides an estimate of the discrepancy in fit per degrees of freedom, and values below · are generally considered as close fit, while values ranging from · to · are accepted as adequate fit (Kline, ).  The p of close fit (PCLOSE) is a one-sided test of the hypothesis that RMSEA is equal to ·.  A significant p value indicates that the model fit is worse than close fitting (i. e.  RMSEA is significantly larger than ·).  Akaike information criterion (AIC) and Browne–Cudeck criterion (BCC) were also considered to further compare the models.  They are likelihood-based measures of model fit that take into account the complexity of models.  When comparing AIC and BCC for multiple models, the smaller the value of the criterion, the better is the model fit to the data.  Fit indices for the models are listed in Table .  Results of the path analyses on Models  to  (Figure ) showed that.  (i) auditory perception was significantly related to phonological processing abilities even after controlling for word recognition, age, and IQ.  (ii) auditory perception did not predict spoken word recognition.  and (iii) spoken word recognition was not significantly related to phonological processing.  Fit indices revealed that Model , with phonological processing predicting both auditory perception and word recognition, provided a better fit to our data than Models  and , though none of these models showed values of CFI and IFI higher than ·.  These results argue against a possible mediating effect of spoken word recognition between auditory perception and phonological processing.  Analogous models (Figure , Models –) were tested with oral language specified by the three subtests (Grammar, Expressive Vocabulary, and Narrative-retell).  Fit indices for all three models indicated relatively good fit to the data, and both AIC and BCC were slightly lower for Model  as compared to Models  and  (Table ).  Spoken word recognition was a significant predictor of oral language in Model , but auditory perception did not predict either word recognition or oral language, and no mediating effect was found.  When the direction of prediction was reversed, oral language significantly predicted both word recognition and auditory perception.  <Conclusion> DISCUSSION.  This study compared SLI and age-matched controls on speech-gating, frequency modulation (FM) detection, and frequency discrimination (FD).  We examined whether SLI is characterized by poorer overall performance on the FD, FM, or speech-gating tasks, and whether spoken word recognition – as measured by speech-gating – mediates the relation between auditory perception and phonological processing.  Speech-gating.  Between-group comparisons in speech-gating yielded several new findings.  First, spoken word recognition developed in the SLI children according to similar pressure constraints as in typically developing children.  Children with SLI demonstrated more expedient word recognition for high- than low-frequency words, and for high- than low-density words.  It therefore appears that the basic process of spoken word recognition does not differ markedly between children with and without SLI.  These findings are in line with findings on another clinical group, namely late talkers.  Although late-talking toddlers’ productive vocabularies are smaller than their age-matched peers’, they tend to produce words with high rather than low neighborhood density – just like their language-matched controls (Stokes, ).  The key finding from speech-gating was that, as a group, children with SLI needed to hear significantly larger portions of high-neighborhood-density words than language-typical controls to both initially identify them, and identify them without subsequent change of mind.  How might poorer word recognition for high-density words in SLI be explained.  We tried to rule out obvious candidates. 